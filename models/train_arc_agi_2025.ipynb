{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97ecff5",
   "metadata": {},
   "source": [
    "# ARC-AGI 2025: Training Notebook\n",
    "\n",
    "This notebook trains a baseline sequence-to-sequence Transformer on the generated ARC-AGI dataset (`artifacts/datasets/*.jsonl`).\n",
    "\n",
    "Sections:\n",
    "1. Setup\n",
    "2. Dependencies\n",
    "3. Device & Determinism\n",
    "4. Load Dataset\n",
    "5. Visualize Samples\n",
    "6. Tokenization & Augmentations\n",
    "7. Datasets & DataLoaders\n",
    "8. Transformer Model\n",
    "9. Loss/Optimizer/Scheduler\n",
    "10. Training Loop\n",
    "11. Validation Metrics\n",
    "12. Inference Solver\n",
    "13. Save Artifacts\n",
    "14. Unit Tests\n",
    "15. Hyperparameter Sweep (optional)\n",
    "16. Export to TorchScript/ONNX (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14e703dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/aibe/Documents/Code/arc-agi\n",
      "Datasets dir: /home/aibe/Documents/Code/arc-agi/artifacts/datasets\n",
      "Run dir: /home/aibe/Documents/Code/arc-agi/models/run_20250809-215720\n"
     ]
    }
   ],
   "source": [
    "# Set Up Environment and Paths\n",
    "from __future__ import annotations\n",
    "import os, json, time, random\n",
    "from pathlib import Path\n",
    "\n",
    "# Robust project root detection for notebooks (no __file__)\n",
    "CWD = Path.cwd()\n",
    "CANDIDATES = [CWD, *CWD.parents]\n",
    "PROJECT_ROOT = None\n",
    "for p in CANDIDATES:\n",
    "    if (p / 'artifacts').exists() and (p / 'models').exists():\n",
    "        PROJECT_ROOT = p\n",
    "        break\n",
    "if PROJECT_ROOT is None:\n",
    "    # Fallback to two levels up from CWD\n",
    "    PROJECT_ROOT = CWD if (CWD / 'artifacts').exists() else CWD.parent\n",
    "\n",
    "DATASETS_DIR = PROJECT_ROOT / 'artifacts' / 'datasets'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "run_id = time.strftime('%Y%m%d-%H%M%S')\n",
    "RUN_DIR = MODELS_DIR / f'run_{run_id}'\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Project root:', PROJECT_ROOT)\n",
    "print('Datasets dir:', DATASETS_DIR)\n",
    "print('Run dir:', RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1146d29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.13.5 (main, Jun 21 2025, 09:35:00) [GCC 15.1.1 20250425]\n",
      "Torch 2.8.0+cu128\n",
      "NumPy 2.3.2\n"
     ]
    }
   ],
   "source": [
    "# Install and Import Dependencies\n",
    "import sys\n",
    "\n",
    "# Optional: install heavy deps if missing\n",
    "try:\n",
    "    import torch\n",
    "except Exception:\n",
    "    %pip install torch --quiet\n",
    "    import torch\n",
    "\n",
    "try:\n",
    "    import einops\n",
    "except Exception:\n",
    "    %pip install einops --quiet\n",
    "    import einops\n",
    "\n",
    "try:\n",
    "    import tqdm\n",
    "except Exception:\n",
    "    %pip install tqdm --quiet\n",
    "    import tqdm\n",
    "\n",
    "try:\n",
    "    import matplotlib\n",
    "except Exception:\n",
    "    %pip install matplotlib --quiet\n",
    "    import matplotlib\n",
    "\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from einops import rearrange\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Python', sys.version)\n",
    "print('Torch', torch.__version__)\n",
    "print('NumPy', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9674c471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Detect Device and Configure Determinism\n",
    "SEED = 1337\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "    except Exception:\n",
    "        pass\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "919a5421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 3232 | Val samples: 358\n"
     ]
    }
   ],
   "source": [
    "# Load ARC-AGI Dataset (from artifacts/datasets/*.jsonl)\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Sample:\n",
    "    split: str\n",
    "    task_id: str\n",
    "    subset: str\n",
    "    index: int\n",
    "    input: List[List[int]]\n",
    "    output: List[List[int]]\n",
    "    transform: dict\n",
    "\n",
    "\n",
    "def read_jsonl(path: Path):\n",
    "    with path.open('r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                yield json.loads(line)\n",
    "\n",
    "\n",
    "def load_split(name: str):\n",
    "    path = DATASETS_DIR / f'{name}.jsonl'\n",
    "    if not path.exists():\n",
    "        print(f'Warning: dataset not found: {path}')\n",
    "        return []\n",
    "    data = []\n",
    "    for rec in read_jsonl(path):\n",
    "        if 'input' in rec and 'output' in rec:\n",
    "            data.append(Sample(\n",
    "                split=rec['split'], task_id=rec['task_id'], subset=rec['subset'], index=rec['index'],\n",
    "                input=rec['input'], output=rec['output'], transform=rec.get('transform', {})\n",
    "            ))\n",
    "    return data\n",
    "\n",
    "train_samples = load_split('training')\n",
    "val_samples = load_split('evaluation')  # use evaluation as validation if present\n",
    "if not val_samples and len(train_samples) > 10:\n",
    "    n = int(0.9 * len(train_samples))\n",
    "    val_samples = train_samples[n:]\n",
    "    train_samples = train_samples[:n]\n",
    "\n",
    "print(f'Train samples: {len(train_samples)} | Val samples: {len(val_samples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c4e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Sample Tasks\n",
    "from itertools import islice\n",
    "\n",
    "def show_grid(ax, grid, title=\"\"):\n",
    "    arr = np.array(grid, dtype=int)\n",
    "    cmap = plt.get_cmap('tab10', 10)\n",
    "    ax.imshow(arr, cmap=cmap, vmin=0, vmax=9)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(10,5))\n",
    "axes = axes.ravel()\n",
    "for i, s in enumerate(islice(train_samples, 4)):\n",
    "    show_grid(axes[2*i], s.input, f\"Train Input {i}\")\n",
    "    show_grid(axes[2*i+1], s.output, f\"Train Output {i}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a8701ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 14\n"
     ]
    }
   ],
   "source": [
    "# Encode Grids to Tokens and Augmentations\n",
    "PAD, BOS, EOS, SEP = 10, 11, 12, 13\n",
    "VOCAB_SIZE = 14  # 0-9 colors + 4 specials\n",
    "\n",
    "def normalize_grid(grid: List[List[int]] | List[int] | int) -> np.ndarray:\n",
    "    arr = np.array(grid, dtype=int)\n",
    "    if arr.ndim == 0:\n",
    "        arr = arr.reshape(1, 1)\n",
    "    elif arr.ndim == 1:\n",
    "        arr = arr.reshape(1, -1)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def encode_grid(grid: List[List[int]] | List[int] | int) -> List[int]:\n",
    "    arr = normalize_grid(grid)\n",
    "    return arr.reshape(-1).tolist()\n",
    "\n",
    "def decode_grid(tokens: List[int], h: int, w: int) -> List[List[int]]:\n",
    "    seq = tokens[: h*w]\n",
    "    return [seq[i*w:(i+1)*w] for i in range(h)]\n",
    "\n",
    "AUG_ROT = [0, 1, 2, 3]\n",
    "AUG_FLIP = [False, True]\n",
    "\n",
    "def apply_aug(grid):\n",
    "    arr = normalize_grid(grid)\n",
    "    # Only apply geometric augs when >0 dims\n",
    "    if arr.shape[0] > 0 and arr.shape[1] > 0:\n",
    "        k = random.choice(AUG_ROT)\n",
    "        if k:\n",
    "            arr = np.rot90(arr, k)\n",
    "        if random.choice(AUG_FLIP):\n",
    "            arr = np.fliplr(arr)\n",
    "        # random color permutation over observed colors\n",
    "        vals = sorted(set(arr.ravel().tolist()))\n",
    "        if len(vals) > 1:\n",
    "            perm = vals[:]\n",
    "            random.shuffle(perm)\n",
    "            mp = {a:b for a,b in zip(vals, perm)}\n",
    "            vfunc = np.vectorize(lambda x: mp.get(int(x), int(x)))\n",
    "            arr = vfunc(arr)\n",
    "    return arr.astype(int).tolist()\n",
    "\n",
    "print('Vocab size:', VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f43ff0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 23)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch Dataset and DataLoaders\n",
    "MAX_H, MAX_W = 30, 30  # ARC grids are typically <= 30\n",
    "\n",
    "def to_2d(grid):\n",
    "    if grid is None:\n",
    "        return []\n",
    "    if isinstance(grid, (int, np.integer)):\n",
    "        return [[int(grid)]]\n",
    "    if isinstance(grid, list):\n",
    "        if not grid:\n",
    "            return []\n",
    "        if isinstance(grid[0], list):\n",
    "            return grid\n",
    "        else:\n",
    "            return [grid]\n",
    "    arr = np.array(grid)\n",
    "    if arr.ndim == 0:\n",
    "        return [[int(arr)]]\n",
    "    if arr.ndim == 1:\n",
    "        return [arr.astype(int).tolist()]\n",
    "    return arr.astype(int).tolist()\n",
    "\n",
    "class ArcSeqDataset(Dataset):\n",
    "    def __init__(self, samples, augment=False):\n",
    "        self.samples = samples\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        inp = to_2d(s.input)\n",
    "        out = to_2d(s.output)\n",
    "        if self.augment:\n",
    "            inp = apply_aug(inp)\n",
    "            out = apply_aug(out)\n",
    "        h_in, w_in = len(inp), len(inp[0]) if inp and len(inp) > 0 else 0\n",
    "        h_out, w_out = len(out), len(out[0]) if out and len(out) > 0 else 0\n",
    "        enc = encode_grid(inp)\n",
    "        dec_tgt_core = encode_grid(out)\n",
    "        dec_tgt = dec_tgt_core + [EOS]  # add EOS for decoding to stop\n",
    "        dec_in = [BOS] + dec_tgt[:-1]\n",
    "        return {\n",
    "            'enc': torch.tensor(enc, dtype=torch.long),\n",
    "            'dec_in': torch.tensor(dec_in, dtype=torch.long),\n",
    "            'tgt': torch.tensor(dec_tgt, dtype=torch.long),\n",
    "            'h_in': h_in, 'w_in': w_in, 'h_out': h_out, 'w_out': w_out\n",
    "        }\n",
    "\n",
    "\n",
    "def make_row_col_indices(h, w):\n",
    "    rows = np.repeat(np.arange(h), w) if (h > 0 and w > 0) else np.array([], dtype=int)\n",
    "    cols = np.tile(np.arange(w), h) if (h > 0 and w > 0) else np.array([], dtype=int)\n",
    "    return rows, cols\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    B = len(batch)\n",
    "    enc_lens = [len(b['enc']) for b in batch]\n",
    "    dec_lens = [len(b['dec_in']) for b in batch]\n",
    "    max_enc = max(enc_lens) if enc_lens else 0\n",
    "    max_dec = max(dec_lens) if dec_lens else 0\n",
    "\n",
    "    enc = torch.full((B, max_enc), PAD, dtype=torch.long)\n",
    "    dec_in = torch.full((B, max_dec), PAD, dtype=torch.long)\n",
    "    tgt = torch.full((B, max_dec), PAD, dtype=torch.long)\n",
    "\n",
    "    enc_pad_mask = torch.ones((B, max_enc), dtype=torch.bool)  # True for pad\n",
    "    dec_pad_mask = torch.ones((B, max_dec), dtype=torch.bool)\n",
    "\n",
    "    row_idx = torch.zeros((B, max_enc), dtype=torch.long)\n",
    "    col_idx = torch.zeros((B, max_enc), dtype=torch.long)\n",
    "\n",
    "    meta = []\n",
    "    for i, b in enumerate(batch):\n",
    "        L_e = len(b['enc']); L_d = len(b['dec_in'])\n",
    "        enc[i, :L_e] = b['enc']\n",
    "        dec_in[i, :L_d] = b['dec_in']\n",
    "        tgt[i, :len(b['tgt'])] = b['tgt']\n",
    "        enc_pad_mask[i, :L_e] = False\n",
    "        dec_pad_mask[i, :L_d] = False\n",
    "        r, c = make_row_col_indices(b['h_in'], b['w_in'])\n",
    "        if L_e > 0 and len(r) == L_e:\n",
    "            row_idx[i, :L_e] = torch.tensor(r, dtype=torch.long)\n",
    "            col_idx[i, :L_e] = torch.tensor(c, dtype=torch.long)\n",
    "        meta.append((b['h_in'], b['w_in'], b['h_out'], b['w_out']))\n",
    "\n",
    "    return {\n",
    "        'enc': enc, 'dec_in': dec_in, 'tgt': tgt,\n",
    "        'enc_pad_mask': enc_pad_mask, 'dec_pad_mask': dec_pad_mask,\n",
    "        'row_idx': row_idx, 'col_idx': col_idx, 'meta': meta\n",
    "    }\n",
    "\n",
    "train_ds = ArcSeqDataset(train_samples, augment=True)\n",
    "val_ds = ArcSeqDataset(val_samples, augment=False)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEM = (device.type == 'cuda')\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEM, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=PIN_MEM, collate_fn=collate_batch)\n",
    "\n",
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a50e8d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transformer Model for ARC\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=4096):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0), persistent=False)\n",
    "    def forward(self, x):\n",
    "        L = x.size(1)\n",
    "        return x + self.pe[:, :L]\n",
    "\n",
    "class ArcTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, d_model=128, nhead=4, num_layers=3, dim_ff=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.row_emb = nn.Embedding(64, d_model)\n",
    "        self.col_emb = nn.Embedding(64, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_ff, dropout, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def encode(self, enc_tokens, row_idx, col_idx, src_key_padding_mask=None):\n",
    "        x = self.tok_emb(enc_tokens) + self.row_emb(row_idx) + self.col_emb(col_idx)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        return x\n",
    "\n",
    "    def decode(self, dec_tokens, memory, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        y = self.tok_emb(dec_tokens)\n",
    "        y = self.pos_enc(y)\n",
    "        L = y.size(1)\n",
    "        causal_mask = torch.triu(torch.ones(L, L, device=y.device, dtype=torch.bool), diagonal=1)\n",
    "        y = self.decoder(y, memory, tgt_mask=causal_mask,\n",
    "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                         memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return self.proj(y)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        memory = self.encode(batch['enc'], batch['row_idx'], batch['col_idx'], src_key_padding_mask=batch['enc_pad_mask'])\n",
    "        logits = self.decode(batch['dec_in'], memory,\n",
    "                             tgt_key_padding_mask=batch['dec_pad_mask'],\n",
    "                             memory_key_padding_mask=batch['enc_pad_mask'])\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, enc, row_idx, col_idx, enc_pad_mask, max_len=256):\n",
    "        self.eval()\n",
    "        memory = self.encode(enc, row_idx, col_idx, src_key_padding_mask=enc_pad_mask)\n",
    "        B = enc.size(0)\n",
    "        ys = torch.full((B, 1), BOS, dtype=torch.long, device=enc.device)\n",
    "        for _ in range(max_len):\n",
    "            logits = self.decode(ys, memory,\n",
    "                                 tgt_key_padding_mask=torch.zeros_like(ys, dtype=torch.bool),\n",
    "                                 memory_key_padding_mask=enc_pad_mask)\n",
    "            next_tok = logits[:, -1].argmax(-1, keepdim=True)\n",
    "            ys = torch.cat([ys, next_tok], dim=1)\n",
    "            if (next_tok == EOS).all():\n",
    "                break\n",
    "        return ys[:, 1:]  # drop BOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dfee4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.013774 M params\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_526498/3418346925.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
     ]
    }
   ],
   "source": [
    "# Configure Loss, Optimizer, and Scheduler\n",
    "model = ArcTransformer().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b9ce7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [train]: 100%|██████████| 202/202 [17:00<00:00,  5.05s/it, loss=0.857]\n",
      "Epoch 1 [train]: 100%|██████████| 202/202 [17:00<00:00,  5.05s/it, loss=0.857]\n",
      "Epoch 1 [val]: 100%|██████████| 23/23 [00:18<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 val_loss=0.8127\n",
      "Saved new best to /home/aibe/Documents/Code/arc-agi/models/run_20250809-185104/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Loop with Mixed Precision and Checkpointing\n",
    "EPOCHS = 1\n",
    "ACCUM_STEPS = 1\n",
    "BEST_VAL = float('inf')\n",
    "\n",
    "best_path = RUN_DIR / 'best.pt'\n",
    "last_path = RUN_DIR / 'last.pt'\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch} [train]')\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step, batch in enumerate(pbar, 1):\n",
    "        for k in ['enc','dec_in','tgt','enc_pad_mask','dec_pad_mask','row_idx','col_idx']:\n",
    "            batch[k] = batch[k].to(device)\n",
    "        with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):\n",
    "            logits = model(batch)\n",
    "            B, L, V = logits.shape\n",
    "            loss = criterion(logits.view(B*L, V), batch['tgt'].view(B*L)) / ACCUM_STEPS\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if step % ACCUM_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        total_loss += loss.item() * ACCUM_STEPS\n",
    "        pbar.set_postfix(loss=total_loss/step)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f'Epoch {epoch} [val]'):\n",
    "            for k in ['enc','dec_in','tgt','enc_pad_mask','dec_pad_mask','row_idx','col_idx']:\n",
    "                batch[k] = batch[k].to(device)\n",
    "            logits = model(batch)\n",
    "            B, L, V = logits.shape\n",
    "            loss = criterion(logits.view(B*L, V), batch['tgt'].view(B*L))\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= max(1, len(val_loader))\n",
    "    print(f'Epoch {epoch} val_loss={val_loss:.4f}')\n",
    "\n",
    "    # Checkpoint\n",
    "    torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(),\n",
    "                'epoch': epoch}, last_path)\n",
    "    if val_loss < BEST_VAL:\n",
    "        BEST_VAL = val_loss\n",
    "        torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(),\n",
    "                    'epoch': epoch}, best_path)\n",
    "        print('Saved new best to', best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f968054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 23/23 [00:16<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 0.0, 'cell_accuracy': 0.724667657335636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate and Compute ARC Metrics\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    exact = 0\n",
    "    total = 0\n",
    "    cell_correct = 0\n",
    "    cell_total = 0\n",
    "    for batch in tqdm(loader, desc='Eval'): \n",
    "        for k in ['enc','dec_in','tgt','enc_pad_mask','dec_pad_mask','row_idx','col_idx']:\n",
    "            batch[k] = batch[k].to(device)\n",
    "        logits = model(batch)\n",
    "        preds = logits.argmax(-1)\n",
    "        mask = batch['tgt'] != PAD\n",
    "        equal = (preds == batch['tgt']) & mask\n",
    "        cell_correct += equal.sum().item()\n",
    "        cell_total += mask.sum().item()\n",
    "        # exact match per sequence\n",
    "        seq_equal = (equal.sum(dim=1) == mask.sum(dim=1))\n",
    "        exact += seq_equal.sum().item()\n",
    "        total += preds.size(0)\n",
    "    return {\n",
    "        'exact_match': exact / max(1, total),\n",
    "        'cell_accuracy': cell_correct / max(1, cell_total)\n",
    "    }\n",
    "\n",
    "metrics = evaluate(model, val_loader)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6189452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: Solve Unseen Tasks\n",
    "@torch.no_grad()\n",
    "def solve_batch(model, batch, max_len=256):\n",
    "    for k in ['enc','enc_pad_mask','row_idx','col_idx']:\n",
    "        batch[k] = batch[k].to(device)\n",
    "    gen = model.generate(batch['enc'], batch['row_idx'], batch['col_idx'], batch['enc_pad_mask'], max_len=max_len)\n",
    "    preds = gen.cpu().numpy().tolist()\n",
    "    outputs = []\n",
    "    for i, (h_in, w_in, h_out, w_out) in enumerate(batch['meta']):\n",
    "        outputs.append(decode_grid(preds[i], h_out, w_out))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31b0179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Artifacts to models/\n",
    "config = {\n",
    "    'model': 'ArcTransformer', 'vocab_size': VOCAB_SIZE,\n",
    "    'd_model': 256, 'nhead': 8, 'num_layers': 4, 'dim_ff': 512,\n",
    "    'batch_size': BATCH_SIZE, 'epochs': EPOCHS, 'seed': SEED,\n",
    "}\n",
    "\n",
    "metrics = evaluate(model, val_loader)\n",
    "with (RUN_DIR / 'metrics.json').open('w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "torch.save({'model': model.state_dict(), 'config': config}, RUN_DIR / 'model.pt')\n",
    "with (RUN_DIR / 'config.json').open('w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print('Saved to', RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c11329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight Unit Tests\n",
    "# 1) Encode/Decode roundtrip\n",
    "_grid = [[1,2,3],[4,5,6]]\n",
    "assert decode_grid(encode_grid(_grid), 2, 3) == _grid\n",
    "print('Encode/Decode test passed')\n",
    "\n",
    "# 2) Batch forward pass sanity\n",
    "batch = next(iter(train_loader))\n",
    "for k in ['enc','dec_in','tgt','enc_pad_mask','dec_pad_mask','row_idx','col_idx']:\n",
    "    batch[k] = batch[k].to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(batch)\n",
    "assert logits.shape[:2] == batch['tgt'].shape\n",
    "print('Forward pass shape test passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32aeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Hyperparameter Sweep Hook\n",
    "from itertools import product\n",
    "\n",
    "def sweep(grid):\n",
    "    results = []\n",
    "    for (lr, layers, heads, dropout) in product(grid['lr'], grid['layers'], grid['heads'], grid['dropout']):\n",
    "        m = ArcTransformer(num_layers=layers, nhead=heads).to(device)\n",
    "        opt = torch.optim.AdamW(m.parameters(), lr=lr)\n",
    "        # One minibatch quick score\n",
    "        batch = next(iter(train_loader))\n",
    "        for k in ['enc','dec_in','tgt','enc_pad_mask','dec_pad_mask','row_idx','col_idx']:\n",
    "            batch[k] = batch[k].to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = m(batch)\n",
    "            B, L, V = logits.shape\n",
    "            loss = criterion(logits.view(B*L, V), batch['tgt'].view(B*L)).item()\n",
    "        results.append({'lr': lr, 'layers': layers, 'heads': heads, 'dropout': dropout, 'loss': loss})\n",
    "    return sorted(results, key=lambda x: x['loss'])\n",
    "\n",
    "# Example sweep grid (commented)\n",
    "# grid = {'lr':[1e-4,3e-4], 'layers':[3,4], 'heads':[4,8], 'dropout':[0.0,0.1]}\n",
    "# sweep_results = sweep(grid)\n",
    "# sweep_results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Export to TorchScript/ONNX\n",
    "try:\n",
    "    example = next(iter(val_loader))\n",
    "    for k in ['enc','dec_in','tgt','enc_pad_mask','dec_pad_mask','row_idx','col_idx']:\n",
    "        example[k] = example[k].to(device)\n",
    "    ts_path = RUN_DIR / 'model_ts.pt'\n",
    "    scripted = torch.jit.trace(model, (example))  # may fail for dict input\n",
    "    scripted.save(str(ts_path))\n",
    "    print('Saved TorchScript to', ts_path)\n",
    "except Exception as e:\n",
    "    print('TorchScript export skipped:', e)\n",
    "\n",
    "try:\n",
    "    import onnx\n",
    "    onnx_path = RUN_DIR / 'model.onnx'\n",
    "    # ONNX export with dynamic axes is non-trivial for dict inputs; skipping here\n",
    "    print('ONNX export not implemented in this baseline')\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "049f64a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval(gen):  74%|███████▍  | 17/23 [2:01:37<42:55, 429.24s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     41\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mexact_match_seq\u001b[39m\u001b[33m'\u001b[39m: exact / \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, total),\n\u001b[32m     42\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcell_accuracy_seq\u001b[39m\u001b[33m'\u001b[39m: cell_correct / \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, cell_total)\n\u001b[32m     43\u001b[39m     }\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Run generate-based eval on validation loader\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m gen_metrics = \u001b[43mevaluate_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(gen_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/arc-agi/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mevaluate_generate\u001b[39m\u001b[34m(model, loader, max_len_factor)\u001b[39m\n\u001b[32m     18\u001b[39m gen_max = [\u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, h_out*w_out) + \u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (_, _, h_out, w_out) \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[33m'\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     19\u001b[39m max_len = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmax\u001b[39m(gen_max) * max_len_factor)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m gen = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_pad_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m preds = gen.cpu().tolist()\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Compare with targets available in batch['tgt']\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/arc-agi/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mArcTransformer.generate\u001b[39m\u001b[34m(self, enc, row_idx, col_idx, enc_pad_mask, max_len)\u001b[39m\n\u001b[32m     58\u001b[39m ys = torch.full((B, \u001b[32m1\u001b[39m), BOS, dtype=torch.long, device=enc.device)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43menc_pad_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     next_tok = logits[:, -\u001b[32m1\u001b[39m].argmax(-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     64\u001b[39m     ys = torch.cat([ys, next_tok], dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mArcTransformer.decode\u001b[39m\u001b[34m(self, dec_tokens, memory, tgt_key_padding_mask, memory_key_padding_mask)\u001b[39m\n\u001b[32m     39\u001b[39m L = y.size(\u001b[32m1\u001b[39m)\n\u001b[32m     40\u001b[39m causal_mask = torch.triu(torch.ones(L, L, device=y.device, dtype=torch.bool), diagonal=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proj(y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/arc-agi/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/arc-agi/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/arc-agi/.venv/lib/python3.13/site-packages/torch/nn/modules/transformer.py:628\u001b[39m, in \u001b[36mTransformerDecoder.forward\u001b[39m\u001b[34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[39m\n\u001b[32m    625\u001b[39m tgt_is_causal = _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m     output = \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    640\u001b[39m     output = \u001b[38;5;28mself\u001b[39m.norm(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/arc-agi/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/arc-agi/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/arc-agi/.venv/lib/python3.13/site-packages/torch/nn/modules/transformer.py:1123\u001b[39m, in \u001b[36mTransformerDecoderLayer.forward\u001b[39m\u001b[34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[39m\n\u001b[32m   1120\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m._ff_block(\u001b[38;5;28mself\u001b[39m.norm3(x))\n\u001b[32m   1121\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1122\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm1(\n\u001b[32m-> \u001b[39m\u001b[32m1123\u001b[39m         x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1124\u001b[39m     )\n\u001b[32m   1125\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm2(\n\u001b[32m   1126\u001b[39m         x\n\u001b[32m   1127\u001b[39m         + \u001b[38;5;28mself\u001b[39m._mha_block(\n\u001b[32m   1128\u001b[39m             x, memory, memory_mask, memory_key_padding_mask, memory_is_causal\n\u001b[32m   1129\u001b[39m         )\n\u001b[32m   1130\u001b[39m     )\n\u001b[32m   1131\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm3(x + \u001b[38;5;28mself\u001b[39m._ff_block(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/arc-agi/.venv/lib/python3.13/site-packages/torch/nn/modules/transformer.py:1143\u001b[39m, in \u001b[36mTransformerDecoderLayer._sa_block\u001b[39m\u001b[34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sa_block\u001b[39m(\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1138\u001b[39m     x: Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1141\u001b[39m     is_causal: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1142\u001b[39m ) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1143\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m   1152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dropout1(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/arc-agi/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/arc-agi/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/arc-agi/.venv/lib/python3.13/site-packages/torch/nn/modules/activation.py:1320\u001b[39m, in \u001b[36mMultiheadAttention.forward\u001b[39m\u001b[34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   1315\u001b[39m         merged_mask, mask_type = \u001b[38;5;28mself\u001b[39m.merge_masks(\n\u001b[32m   1316\u001b[39m             attn_mask, key_padding_mask, query\n\u001b[32m   1317\u001b[39m         )\n\u001b[32m   1319\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.in_proj_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_native_multi_head_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m                \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m                \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m                \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmerged_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m                \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m                \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1336\u001b[39m any_nested = query.is_nested \u001b[38;5;129;01mor\u001b[39;00m key.is_nested \u001b[38;5;129;01mor\u001b[39;00m value.is_nested\n\u001b[32m   1337\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m any_nested, (\n\u001b[32m   1338\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMultiheadAttention does not support NestedTensor outside of its fast path. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1339\u001b[39m     + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe fast path was not hit because \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhy_not_fast_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1340\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Generate-and-Compare Evaluation (exact grid match)\n",
    "@torch.no_grad()\n",
    "def evaluate_generate(model, loader, max_len_factor=1.2):\n",
    "    model.eval()\n",
    "    exact = 0\n",
    "    total = 0\n",
    "    cell_correct = 0\n",
    "    cell_total = 0\n",
    "\n",
    "    for batch in tqdm(loader, desc='Eval(gen)'):\n",
    "        # Move encoder-side inputs to device\n",
    "        enc = batch['enc'].to(device)\n",
    "        row_idx = batch['row_idx'].to(device)\n",
    "        col_idx = batch['col_idx'].to(device)\n",
    "        enc_pad_mask = batch['enc_pad_mask'].to(device)\n",
    "\n",
    "        # Determine per-sample max length (H*W + 1 for EOS)\n",
    "        gen_max = [max(1, h_out*w_out) + 1 for (_, _, h_out, w_out) in batch['meta']]\n",
    "        max_len = int(max(gen_max) * max_len_factor)\n",
    "\n",
    "        gen = model.generate(enc, row_idx, col_idx, enc_pad_mask, max_len=max_len)\n",
    "        preds = gen.cpu().tolist()\n",
    "\n",
    "        # Compare with targets available in batch['tgt']\n",
    "        tgt = batch['tgt']\n",
    "        pad_mask = (tgt != PAD)\n",
    "        # Trim preds to tgt length\n",
    "        for i in range(tgt.size(0)):\n",
    "            tlen = pad_mask[i].sum().item()\n",
    "            p = preds[i][:tlen]\n",
    "            t = tgt[i, :tlen].tolist()\n",
    "            # Per-cell\n",
    "            cell_total += tlen\n",
    "            cell_correct += sum(int(pi == ti) for pi, ti in zip(p, t))\n",
    "            # Exact sequence (including EOS)\n",
    "            if p == t:\n",
    "                exact += 1\n",
    "            total += 1\n",
    "\n",
    "    return {\n",
    "        'exact_match_seq': exact / max(1, total),\n",
    "        'cell_accuracy_seq': cell_correct / max(1, cell_total)\n",
    "    }\n",
    "\n",
    "# Run generate-based eval on validation loader\n",
    "gen_metrics = evaluate_generate(model, val_loader)\n",
    "print(gen_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f1ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d236390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== ArcSeqDataset ==\n",
      "Error getting ArcSeqDataset source: source code not available\n",
      "\n",
      "== collate_fn or collate ==\n",
      "\n",
      "-- collate_batch --\n",
      "\n",
      "def collate_batch(batch):\n",
      "    B = len(batch)\n",
      "    enc_lens = [len(b['enc']) for b in batch]\n",
      "    dec_lens = [len(b['dec_in']) for b in batch]\n",
      "    max_enc = max(enc_lens) if enc_lens else 0\n",
      "    max_dec = max(dec_lens) if dec_lens else 0\n",
      "\n",
      "    enc = torch.full((B, max_enc), PAD, dtype=torch.long)\n",
      "    dec_in = torch.full((B, max_dec), PAD, dtype=torch.long)\n",
      "    tgt = torch.full((B, max_dec), PAD, dtype=torch.long)\n",
      "\n",
      "    enc_pad_mask = torch.ones((B, max_enc), dtype=torch.bool)  # True for pad\n",
      "    dec_pad_mask = torch.ones((B, max_dec), dtype=torch.bool)\n",
      "\n",
      "    row_idx = torch.zeros((B, max_enc), dtype=torch.long)\n",
      "    col_idx = torch.zeros((B, max_enc), dtype=torch.long)\n",
      "\n",
      "    meta = []\n",
      "    for i, b in enumerate(batch):\n",
      "        L_e = len(b['enc']); L_d = len(b['dec_in'])\n",
      "        enc[i, :L_e] = b['enc']\n",
      "        dec_in[i, :L_d] = b['dec_in']\n",
      "        tgt[i, :len(b['tgt'])] = b['tgt']\n",
      "        enc_pad_mask[i, :L_e] = False\n",
      "        dec_pad_mask[i, :L_d] = False\n",
      "        r, c = make_row_col_indices(b['h_in'], b['w_in'])\n",
      "        if L_e > 0 and len(r) == L_e:\n",
      "            row_idx[i, :L_e] = torch.tensor(r, dtype=torch.long)\n",
      "            col_idx[i, :L_e] = torch.tensor(c, dtype=torch.long)\n",
      "        meta.append((b['h_in'], b['w_in'], b['h_out'], b['w_out']))\n",
      "\n",
      "    return {\n",
      "        'enc': enc, 'dec_in': dec_in, 'tgt': tgt,\n",
      "        'enc_pad_mask': enc_pad_mask, 'dec_pad_mask': dec_pad_mask,\n",
      "        'row_idx': row_idx, 'col_idx': col_idx, 'meta': meta\n",
      "    }\n",
      "\n",
      "\n",
      "== tokenization helpers (encode/decode) ==\n",
      "\n",
      "-- encode_grid --\n",
      "\n",
      "def encode_grid(grid: List[List[int]] | List[int] | int) -> List[int]:\n",
      "    arr = normalize_grid(grid)\n",
      "    return arr.reshape(-1).tolist()\n",
      "\n",
      "\n",
      "-- decode_grid --\n",
      "\n",
      "def decode_grid(tokens: List[int], h: int, w: int) -> List[List[int]]:\n",
      "    seq = tokens[: h*w]\n",
      "    return [seq[i*w:(i+1)*w] for i in range(h)]\n",
      "\n",
      "\n",
      "== model.generate ==\n",
      "    @torch.no_grad()\n",
      "    def generate(self, enc, row_idx, col_idx, enc_pad_mask, max_len=256):\n",
      "        self.eval()\n",
      "        memory = self.encode(enc, row_idx, col_idx, src_key_padding_mask=enc_pad_mask)\n",
      "        B = enc.size(0)\n",
      "        ys = torch.full((B, 1), BOS, dtype=torch.long, device=enc.device)\n",
      "        for _ in range(max_len):\n",
      "            logits = self.decode(ys, memory,\n",
      "                                 tgt_key_padding_mask=torch.zeros_like(ys, dtype=torch.bool),\n",
      "                                 memory_key_padding_mask=enc_pad_mask)\n",
      "            next_tok = logits[:, -1].argmax(-1, keepdim=True)\n",
      "            ys = torch.cat([ys, next_tok], dim=1)\n",
      "            if (next_tok == EOS).all():\n",
      "                break\n",
      "        return ys[:, 1:]  # drop BOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from textwrap import indent\n",
    "\n",
    "print(\"== ArcSeqDataset ==\")\n",
    "try:\n",
    "    print(inspect.getsource(ArcSeqDataset))\n",
    "except Exception as e:\n",
    "    print(\"Error getting ArcSeqDataset source:\", e)\n",
    "\n",
    "print(\"\\n== collate_fn or collate ==\")\n",
    "for name in list(globals().keys()):\n",
    "    if name.lower().startswith(\"collate\"):\n",
    "        obj = globals()[name]\n",
    "        if callable(obj):\n",
    "            print(f\"\\n-- {name} --\\n\")\n",
    "            try:\n",
    "                print(inspect.getsource(obj))\n",
    "            except Exception as e:\n",
    "                print(\"(no source)\", e)\n",
    "\n",
    "print(\"\\n== tokenization helpers (encode/decode) ==\")\n",
    "for name in [n for n in globals().keys() if any(n.lower().startswith(p) for p in (\"encode\",\"decode\",\"grid_to\",\"seq_to\"))]:\n",
    "    obj = globals()[name]\n",
    "    if callable(obj):\n",
    "        print(f\"\\n-- {name} --\\n\")\n",
    "        try:\n",
    "            print(inspect.getsource(obj))\n",
    "        except Exception as e:\n",
    "            print(\"(no source)\", e)\n",
    "\n",
    "print(\"\\n== model.generate ==\")\n",
    "try:\n",
    "    print(inspect.getsource(model.generate))\n",
    "except Exception as e:\n",
    "    print(\"Error getting model.generate source:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2926c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample 0:\n",
      "keys: ['enc', 'dec_in', 'tgt', 'h_in', 'w_in', 'h_out', 'w_out']\n",
      "enc: <class 'torch.Tensor'> -> tensor([3, 9, 7, 4])\n",
      "dec_in: <class 'torch.Tensor'> -> tensor([11,  9,  3,  9,  3,  9,  3,  7,  4,  7,  4,  7,  4,  3,  9,  3,  9,  3,\n",
      "         9,  4,  7,  4,  7,  4,  7,  9,  3,  9,  3,  9,  3,  7,  4,  7,  4,  7,\n",
      "         4])\n",
      "tgt: <class 'torch.Tensor'> -> tensor([ 9,  3,  9,  3,  9,  3,  7,  4,  7,  4,  7,  4,  3,  9,  3,  9,  3,  9,\n",
      "         4,  7,  4,  7,  4,  7,  9,  3,  9,  3,  9,  3,  7,  4,  7,  4,  7,  4,\n",
      "        12])\n",
      "h_in 2\n",
      "w_in 2\n",
      "h_out 6\n",
      "w_out 6\n",
      "\n",
      "Val sample 0:\n",
      "keys: ['enc', 'dec_in', 'tgt', 'h_in', 'w_in', 'h_out', 'w_out']\n",
      "enc: <class 'torch.Tensor'> -> tensor([3, 5, 3, 3, 6, 6, 5, 4, 1, 4, 9, 9, 4, 3, 9, 9, 9, 9, 3, 4, 9, 9, 4, 1,\n",
      "        4, 5, 6, 6, 3, 3, 5, 3, 3, 3, 6, 6, 4, 5, 4, 1, 9, 9, 3, 4, 9, 1, 1, 9,\n",
      "        4, 3, 9, 9, 1, 4, 5, 4, 6, 6, 3, 3, 1, 1, 3, 5, 5, 4, 6, 6, 9, 1, 1, 4,\n",
      "        9, 9, 4, 5, 5, 4, 9, 9, 4, 1, 1, 9, 6, 6, 4, 5, 5, 3, 1, 1, 5, 3, 4, 5,\n",
      "        6, 6, 1, 9, 4, 1, 9, 1, 4, 4, 4, 4, 1, 9, 1, 4, 9, 1, 6, 6, 5, 4, 3, 5,\n",
      "        6, 9, 9, 9, 3, 5, 3, 3, 4, 3, 9, 9, 9, 2, 6, 9, 9, 6, 2, 9, 9, 9, 3, 4,\n",
      "        3, 3, 5, 3, 9, 9, 9, 6, 9, 9, 5, 3, 3, 3, 3, 4, 9, 1, 9, 9, 9, 6, 6, 9,\n",
      "        9, 9, 1, 9, 4, 3, 3, 3, 3, 5, 9, 9, 9, 9, 6, 9, 1, 1, 3, 5, 9, 9, 4, 4,\n",
      "        6, 9, 9, 2, 2, 9, 9, 6, 4, 4, 9, 9, 5, 3, 1, 1, 9, 6, 9, 9, 9, 6, 1, 1,\n",
      "        5, 3, 9, 1, 5, 4, 9, 6, 9, 9, 9, 9, 6, 9, 4, 5, 1, 9, 3, 5, 1, 1, 6, 9,\n",
      "        1, 4, 9, 1, 4, 3, 9, 9, 5, 5, 7, 2, 4, 3, 2, 4, 4, 2, 3, 4, 2, 7, 5, 5,\n",
      "        9, 9, 3, 4, 1, 9, 4, 1, 1, 9, 3, 4, 9, 1, 4, 5, 2, 7, 3, 4, 4, 2, 2, 4,\n",
      "        4, 3, 7, 2, 5, 4, 1, 9, 4, 3, 9, 1, 9, 9, 1, 4, 9, 9, 4, 5, 6, 4, 5, 5,\n",
      "        2, 4, 4, 3, 3, 4, 4, 2, 5, 5, 4, 6, 5, 4, 9, 9, 4, 1, 9, 9, 4, 1, 9, 1,\n",
      "        4, 4, 4, 5, 4, 5, 4, 2, 3, 4, 4, 3, 2, 4, 5, 4, 5, 4, 4, 4, 1, 9, 1, 4,\n",
      "        4, 3, 9, 9, 9, 9, 6, 9, 5, 9, 7, 7, 5, 5, 7, 2, 2, 7, 5, 5, 7, 7, 9, 5,\n",
      "        9, 6, 9, 9, 9, 9, 3, 4, 9, 1, 2, 9, 9, 6, 9, 5, 7, 7, 4, 5, 2, 7, 7, 2,\n",
      "        5, 4, 7, 7, 5, 9, 6, 9, 9, 2, 1, 9, 9, 9, 4, 4, 6, 9, 9, 9, 7, 7, 5, 9,\n",
      "        5, 4, 5, 5, 5, 5, 4, 5, 9, 5, 7, 7, 9, 8, 8, 8, 8, 4, 9, 1, 5, 4, 9, 6,\n",
      "        2, 9, 7, 7, 9, 5, 4, 6, 4, 5, 5, 4, 6, 4, 5, 9, 7, 7, 9, 8, 8, 8, 8, 5,\n",
      "        9, 1, 5, 4, 9, 6, 2, 9, 7, 7, 9, 5, 4, 6, 4, 5, 5, 4, 6, 4, 5, 9, 7, 7,\n",
      "        9, 8, 8, 8, 8, 5, 9, 9, 4, 4, 6, 9, 9, 9, 7, 7, 5, 9, 5, 4, 5, 5, 5, 5,\n",
      "        4, 5, 9, 5, 7, 7, 9, 8, 8, 8, 8, 4, 3, 4, 9, 1, 2, 9, 9, 6, 9, 5, 7, 7,\n",
      "        4, 5, 2, 7, 7, 2, 5, 4, 7, 7, 5, 9, 6, 8, 8, 8, 8, 9, 4, 3, 9, 9, 9, 9,\n",
      "        6, 9, 5, 9, 7, 7, 5, 5, 7, 2, 2, 7, 5, 5, 7, 7, 9, 5, 9, 8, 8, 8, 8, 9,\n",
      "        9, 9, 4, 1, 9, 1, 4, 4, 4, 5, 4, 5, 4, 2, 3, 4, 4, 3, 2, 4, 5, 4, 5, 4,\n",
      "        4, 8, 8, 8, 8, 4, 9, 9, 1, 4, 9, 9, 4, 5, 6, 4, 5, 5, 2, 4, 4, 3, 3, 4,\n",
      "        4, 2, 5, 5, 4, 6, 5, 8, 8, 8, 8, 1, 4, 1, 1, 9, 3, 4, 9, 1, 4, 5, 2, 7,\n",
      "        3, 4, 4, 2, 2, 4, 4, 3, 7, 2, 5, 4, 1, 8, 8, 8, 8, 1, 1, 4, 9, 1, 4, 3,\n",
      "        9, 9, 5, 5, 7, 2, 4, 3, 2, 4, 4, 2, 3, 4, 2, 7, 5, 5, 9, 9, 3, 4, 1, 9,\n",
      "        9, 9, 9, 6, 1, 1, 5, 3, 9, 1, 5, 4, 9, 6, 9, 9, 9, 9, 6, 9, 4, 5, 1, 9,\n",
      "        3, 5, 1, 1, 6, 9, 9, 9, 6, 9, 1, 1, 3, 5, 9, 9, 4, 4, 6, 9, 9, 2, 2, 9,\n",
      "        9, 6, 4, 4, 9, 9, 5, 3, 1, 1, 9, 6, 9, 6, 9, 9, 5, 3, 3, 3, 3, 4, 9, 1,\n",
      "        9, 9, 9, 6, 6, 9, 9, 9, 1, 9, 4, 3, 3, 3, 3, 5, 9, 9, 6, 9, 9, 9, 3, 5,\n",
      "        3, 3, 4, 3, 9, 9, 9, 2, 6, 9, 9, 6, 2, 9, 9, 9, 3, 4, 3, 3, 5, 3, 9, 9,\n",
      "        1, 1, 5, 3, 4, 5, 6, 6, 1, 9, 4, 1, 9, 1, 4, 4, 4, 4, 1, 9, 1, 4, 9, 1,\n",
      "        6, 6, 5, 4, 3, 5, 1, 1, 3, 5, 5, 4, 6, 6, 9, 1, 1, 4, 9, 9, 4, 5, 5, 4,\n",
      "        9, 9, 4, 1, 1, 9, 6, 6, 4, 5, 5, 3])\n",
      "dec_in: <class 'torch.Tensor'> -> tensor([11,  9,  9,  6,  4,  2,  6,  9,  4,  2,  6,  9,  4,  9,  9,  6,  4,  9,\n",
      "         9,  2,  1,  6,  9,  9,  9,  4,  1,  9,  1,  4,  9,  9,  4,  9,  4,  3,\n",
      "         9])\n",
      "tgt: <class 'torch.Tensor'> -> tensor([ 9,  9,  6,  4,  2,  6,  9,  4,  2,  6,  9,  4,  9,  9,  6,  4,  9,  9,\n",
      "         2,  1,  6,  9,  9,  9,  4,  1,  9,  1,  4,  9,  9,  4,  9,  4,  3,  9,\n",
      "        12])\n",
      "h_in 30\n",
      "w_in 30\n",
      "h_out 9\n",
      "w_out 4\n",
      "\n",
      "train_loader.collate_fn: <function collate_batch at 0x7f5bf52c4220>\n"
     ]
    }
   ],
   "source": [
    "def peek_sample(ds, idx=0):\n",
    "    s = ds[idx]\n",
    "    print(\"keys:\", list(s.keys()))\n",
    "    for k in (\"enc\",\"dec_in\",\"tgt\"):\n",
    "        v = s.get(k)\n",
    "        if isinstance(v, list):\n",
    "            print(f\"{k}: len={len(v)} head={v[:10]}\")\n",
    "        else:\n",
    "            print(f\"{k}: {type(v)} -> {v}\")\n",
    "    for k in (\"h_in\",\"w_in\",\"h_out\",\"w_out\"):\n",
    "        print(k, s.get(k))\n",
    "\n",
    "print(\"Train sample 0:\")\n",
    "peek_sample(train_ds, 0)\n",
    "\n",
    "print(\"\\nVal sample 0:\")\n",
    "peek_sample(val_ds, 0)\n",
    "\n",
    "print(\"\\ntrain_loader.collate_fn:\", getattr(train_loader, 'collate_fn', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f5c2b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len summaries:\n",
      "{'avg_dec_in': 134.4, 'avg_tgt': 134.4, 'avg_area': 133.4, 'eos_present_cnt': 20}\n",
      "First 3: [{'i': 0, 'dec_in': 37, 'tgt': 37, 'area': 36, 'has_eos': tensor(True)}, {'i': 1, 'dec_in': 37, 'tgt': 37, 'area': 36, 'has_eos': tensor(True)}, {'i': 2, 'dec_in': 82, 'tgt': 82, 'area': 81, 'has_eos': tensor(True)}]\n",
      "\n",
      "Val len summaries:\n",
      "{'avg_dec_in': 255.9, 'avg_tgt': 255.9, 'avg_area': 254.9, 'eos_present_cnt': 20}\n",
      "First 3: [{'i': 0, 'dec_in': 37, 'tgt': 37, 'area': 36, 'has_eos': tensor(True)}, {'i': 1, 'dec_in': 21, 'tgt': 21, 'area': 20, 'has_eos': tensor(True)}, {'i': 2, 'dec_in': 22, 'tgt': 22, 'area': 21, 'has_eos': tensor(True)}]\n"
     ]
    }
   ],
   "source": [
    "def has_eos(sample):\n",
    "    return len(sample['tgt']) > 0 and sample['tgt'][-1] == EOS\n",
    "\n",
    "# Inspect first 20 samples from train/val\n",
    "train_checks = []\n",
    "for i in range(min(20, len(train_ds))):\n",
    "    s = train_ds[i]\n",
    "    area = int(s['h_out']) * int(s['w_out'])\n",
    "    train_checks.append({\n",
    "        'i': i,\n",
    "        'dec_in': len(s['dec_in']),\n",
    "        'tgt': len(s['tgt']),\n",
    "        'area': area,\n",
    "        'has_eos': has_eos(s)\n",
    "    })\n",
    "\n",
    "val_checks = []\n",
    "for i in range(min(20, len(val_ds))):\n",
    "    s = val_ds[i]\n",
    "    area = int(s['h_out']) * int(s['w_out'])\n",
    "    val_checks.append({\n",
    "        'i': i,\n",
    "        'dec_in': len(s['dec_in']),\n",
    "        'tgt': len(s['tgt']),\n",
    "        'area': area,\n",
    "        'has_eos': has_eos(s)\n",
    "    })\n",
    "\n",
    "print('Train len summaries:')\n",
    "print({\n",
    "    'avg_dec_in': sum(x['dec_in'] for x in train_checks)/len(train_checks),\n",
    "    'avg_tgt': sum(x['tgt'] for x in train_checks)/len(train_checks),\n",
    "    'avg_area': sum(x['area'] for x in train_checks)/len(train_checks),\n",
    "    'eos_present_cnt': sum(1 for x in train_checks if x['has_eos'])\n",
    "})\n",
    "print('First 3:', train_checks[:3])\n",
    "\n",
    "print('\\nVal len summaries:')\n",
    "print({\n",
    "    'avg_dec_in': sum(x['dec_in'] for x in val_checks)/len(val_checks),\n",
    "    'avg_tgt': sum(x['tgt'] for x in val_checks)/len(val_checks),\n",
    "    'avg_area': sum(x['area'] for x in val_checks)/len(val_checks),\n",
    "    'eos_present_cnt': sum(1 for x in val_checks if x['has_eos'])\n",
    "})\n",
    "print('First 3:', val_checks[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "576807a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best checkpoint: /home/aibe/Documents/Code/arc-agi/models/run_20250809-185104/best.pt\n",
      "Generate eval (val): {'exact_match': 0.0, 'cell_accuracy': 0.27584441142214544, 'n_examples': 358}\n"
     ]
    }
   ],
   "source": [
    "# Load best checkpoint (if exists) and run generation-based evaluation on val\n",
    "import os\n",
    "\n",
    "def load_best(model, path):\n",
    "    if os.path.exists(path):\n",
    "        sd = torch.load(path, map_location=device)\n",
    "        if isinstance(sd, dict) and 'model' in sd:\n",
    "            model.load_state_dict(sd['model'])\n",
    "        else:\n",
    "            model.load_state_dict(sd)\n",
    "        print(f\"Loaded best checkpoint: {path}\")\n",
    "    else:\n",
    "        print(f\"Best checkpoint not found: {path}\")\n",
    "\n",
    "load_best(model, str(best_path))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_generate(model, loader, max_gen_len=None, limit=None):\n",
    "    model.eval()\n",
    "    total_cells = 0\n",
    "    correct_cells = 0\n",
    "    exact_matches = 0\n",
    "    total_examples = 0\n",
    "\n",
    "    for bi, batch in enumerate(loader):\n",
    "        enc = batch['enc'].to(device)\n",
    "        enc_pad = batch['enc_pad_mask'].to(device)\n",
    "        row_idx = batch['row_idx'].to(device)\n",
    "        col_idx = batch['col_idx'].to(device)\n",
    "        tgt = batch['tgt'].to(device)\n",
    "        B = enc.size(0)\n",
    "\n",
    "        # meta: list of (h_in, w_in, h_out, w_out)\n",
    "        metas = batch['meta']\n",
    "        areas = [int(h_out) * int(w_out) for (_, _, h_out, w_out) in metas]\n",
    "        max_area = max(areas) if areas else 0\n",
    "        max_len = max_gen_len or (max_area + 2)  # +EOS margin\n",
    "\n",
    "        ys = model.generate(enc, row_idx, col_idx, enc_pad, max_len=max_len)\n",
    "        # ys: [B, T] including tokens until EOS (BOS dropped in generate)\n",
    "\n",
    "        for i in range(B):\n",
    "            area = areas[i]\n",
    "            # target first area tokens (exclude EOS)\n",
    "            tgt_seq = tgt[i, :area]\n",
    "            # predicted tokens up to EOS (exclude EOS itself if present)\n",
    "            pred = ys[i]\n",
    "            # cut at EOS if present\n",
    "            eos_pos = (pred == EOS).nonzero(as_tuple=False)\n",
    "            if len(eos_pos) > 0:\n",
    "                pred = pred[: int(eos_pos[0].item())]\n",
    "            # compare within area\n",
    "            pred_area = pred[:area]\n",
    "            # pad/truncate predicted area to area length\n",
    "            if pred_area.numel() < area:\n",
    "                pad = torch.full((area - pred_area.numel(),), PAD, dtype=pred_area.dtype, device=pred_area.device)\n",
    "                pred_area = torch.cat([pred_area, pad], dim=0)\n",
    "            else:\n",
    "                pred_area = pred_area[:area]\n",
    "\n",
    "            correct = (pred_area == tgt_seq).sum().item()\n",
    "            correct_cells += correct\n",
    "            total_cells += area\n",
    "            if correct == area and pred.numel() >= area:\n",
    "                # exact if all area cells correct; length check relaxed as long as first area match\n",
    "                exact_matches += 1\n",
    "            total_examples += 1\n",
    "\n",
    "        if limit and total_examples >= limit:\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        'exact_match': exact_matches / max(1, total_examples),\n",
    "        'cell_accuracy': correct_cells / max(1, total_cells),\n",
    "        'n_examples': total_examples,\n",
    "    }\n",
    "\n",
    "metrics_gen = evaluate_generate(model, val_loader, max_gen_len=MAX_H*MAX_W+2)\n",
    "print(\"Generate eval (val):\", metrics_gen)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
