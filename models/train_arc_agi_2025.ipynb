{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97ecff5",
   "metadata": {},
   "source": [
    "# ARC-AGI 2025: Training Notebook\n",
    "\n",
    "This notebook trains a baseline sequence-to-sequence Transformer on the generated ARC-AGI dataset (`artifacts/datasets/*.jsonl`).\n",
    "\n",
    "Sections:\n",
    "1. Setup\n",
    "2. Dependencies\n",
    "3. Device & Determinism\n",
    "4. Load Dataset\n",
    "5. Visualize Samples\n",
    "6. Tokenization & Augmentations\n",
    "7. Datasets & DataLoaders\n",
    "8. Transformer Model\n",
    "9. Loss/Optimizer/Scheduler\n",
    "10. Training Loop\n",
    "11. Validation Metrics\n",
    "12. Inference Solver\n",
    "13. Save Artifacts\n",
    "14. Unit Tests\n",
    "15. Hyperparameter Sweep (optional)\n",
    "16. Export to TorchScript/ONNX (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e703dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\n",
      "Datasets dir: c:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\artifacts\\datasets\n",
      "Run dir: c:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\models\\run_20250824-092914\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, json, time, random\n",
    "from pathlib import Path\n",
    "CWD = Path.cwd()\n",
    "CANDIDATES = [CWD, *CWD.parents]\n",
    "PROJECT_ROOT = None\n",
    "for p in CANDIDATES:\n",
    "    if (p / 'artifacts').exists() and (p / 'models').exists():\n",
    "        PROJECT_ROOT = p\n",
    "        break\n",
    "if PROJECT_ROOT is None:\n",
    "    PROJECT_ROOT = CWD if (CWD / 'artifacts').exists() else CWD.parent\n",
    "DATASETS_DIR = PROJECT_ROOT / 'artifacts' / 'datasets'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "run_id = time.strftime('%Y%m%d-%H%M%S')\n",
    "RUN_DIR = MODELS_DIR / f'run_{run_id}'\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print('Project root:', PROJECT_ROOT)\n",
    "print('Datasets dir:', DATASETS_DIR)\n",
    "print('Run dir:', RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1146d29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.13.7 (tags/v3.13.7:bcee1c3, Aug 14 2025, 14:15:11) [MSC v.1944 64 bit (AMD64)]\n",
      "Torch 2.8.0+cpu\n",
      "NumPy 2.3.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "try:\n",
    "    import torch\n",
    "except Exception:\n",
    "    %pip install torch --quiet\n",
    "    import torch\n",
    "try:\n",
    "    import einops\n",
    "except Exception:\n",
    "    %pip install einops --quiet\n",
    "    import einops\n",
    "try:\n",
    "    import tqdm\n",
    "except Exception:\n",
    "    %pip install tqdm --quiet\n",
    "    import tqdm\n",
    "try:\n",
    "    import matplotlib\n",
    "except Exception:\n",
    "    %pip install matplotlib --quiet\n",
    "    import matplotlib\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from einops import rearrange\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "print('Python', sys.version)\n",
    "print('Torch', torch.__version__)\n",
    "print('NumPy', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9674c471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "SEED = 1337\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "    except Exception:\n",
    "        pass\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "919a5421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 3232 | Val samples: 358\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Sample:\n",
    "    split: str\n",
    "    task_id: str\n",
    "    subset: str\n",
    "    index: int\n",
    "    input: List[List[int]]\n",
    "    output: List[List[int]]\n",
    "    transform: dict\n",
    "\n",
    "def read_jsonl(path: Path):\n",
    "    with path.open('r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                yield json.loads(line)\n",
    "\n",
    "def load_split(name: str):\n",
    "    path = DATASETS_DIR / f'{name}.jsonl'\n",
    "    if not path.exists():\n",
    "        print(f'Warning: dataset not found: {path}')\n",
    "        return []\n",
    "    data = []\n",
    "    for rec in read_jsonl(path):\n",
    "        if 'input' in rec and 'output' in rec:\n",
    "            data.append(Sample(\n",
    "                split=rec['split'], task_id=rec['task_id'], subset=rec['subset'], index=rec['index'],\n",
    "                input=rec['input'], output=rec['output'], transform=rec.get('transform', {})\n",
    "            ))\n",
    "    return data\n",
    "train_samples = load_split('training')\n",
    "val_samples = load_split('evaluation')\n",
    "if not val_samples and len(train_samples) > 10:\n",
    "    n = int(0.9 * len(train_samples))\n",
    "    val_samples = train_samples[n:]\n",
    "    train_samples = train_samples[:n]\n",
    "print(f'Train samples: {len(train_samples)} | Val samples: {len(val_samples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Small Experiment Subset Setup ===\n",
    "# Reduce dataset size for a fast baseline experiment run.\n",
    "MAX_TRAIN = 200           # cap number of training examples considered (set None for full)\n",
    "VAL_RATIO = 0.1           # portion of (post-cap) training examples to use as validation if evaluation split empty\n",
    "FORCE_RESPLIT = True      # force re-splitting even if evaluation examples exist (set False to keep evaluation split)\n",
    "\n",
    "original_train_len = len(train_samples)\n",
    "original_val_len = len(val_samples)\n",
    "\n",
    "# Cap train set\n",
    "if MAX_TRAIN is not None and len(train_samples) > MAX_TRAIN:\n",
    "    train_samples = train_samples[:MAX_TRAIN]\n",
    "\n",
    "# Re-split if evaluation empty or forced\n",
    "if FORCE_RESPLIT or (not val_samples):\n",
    "    cut = int(len(train_samples) * (1 - VAL_RATIO))\n",
    "    if cut <= 0: cut = max(1, len(train_samples) - 1)\n",
    "    val_samples = train_samples[cut:]\n",
    "    train_samples = train_samples[:cut]\n",
    "\n",
    "print(f\"Small experiment: train={len(train_samples)} val={len(val_samples)} (orig train={original_train_len} val={original_val_len})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998c4e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "def show_grid(ax, grid, title=\"\"):\n",
    "    arr = np.array(grid, dtype=int)\n",
    "    cmap = plt.get_cmap('tab10', 10)\n",
    "    ax.imshow(arr, cmap=cmap, vmin=0, vmax=9)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "fig, axes = plt.subplots(2, 4, figsize=(10,5))\n",
    "axes = axes.ravel()\n",
    "for i, s in enumerate(islice(train_samples, 4)):\n",
    "    show_grid(axes[2*i], s.input, f\"Train Input {i}\")\n",
    "    show_grid(axes[2*i+1], s.output, f\"Train Output {i}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a8701ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 14\n"
     ]
    }
   ],
   "source": [
    "PAD, BOS, EOS, SEP = 10, 11, 12, 13\n",
    "VOCAB_SIZE = 14\n",
    "\n",
    "def normalize_grid(grid: List[List[int]] | List[int] | int) -> np.ndarray:\n",
    "    arr = np.array(grid, dtype=int)\n",
    "    if arr.ndim == 0:\n",
    "        arr = arr.reshape(1, 1)\n",
    "    elif arr.ndim == 1:\n",
    "        arr = arr.reshape(1, -1)\n",
    "    return arr\n",
    "\n",
    "def encode_grid(grid: List[List[int]] | List[int] | int) -> List[int]:\n",
    "    arr = normalize_grid(grid)\n",
    "    return arr.reshape(-1).tolist()\n",
    "\n",
    "def decode_grid(tokens: List[int], h: int, w: int) -> List[List[int]]:\n",
    "    seq = tokens[: h*w]\n",
    "    return [seq[i*w:(i+1)*w] for i in range(h)]\n",
    "AUG_ROT = [0, 1, 2, 3]\n",
    "AUG_FLIP = [False, True]\n",
    "\n",
    "def apply_aug(grid):\n",
    "    arr = normalize_grid(grid)\n",
    "    if arr.shape[0] > 0 and arr.shape[1] > 0:\n",
    "        k = random.choice(AUG_ROT)\n",
    "        if k:\n",
    "            arr = np.rot90(arr, k)\n",
    "        if random.choice(AUG_FLIP):\n",
    "            arr = np.fliplr(arr)\n",
    "        vals = sorted(set(arr.ravel().tolist()))\n",
    "        if len(vals) > 1:\n",
    "            perm = vals[:]\n",
    "            random.shuffle(perm)\n",
    "            mp = {a:b for a,b in zip(vals, perm)}\n",
    "            vfunc = np.vectorize(lambda x: mp.get(int(x), int(x)))\n",
    "            arr = vfunc(arr)\n",
    "    return arr.astype(int).tolist()\n",
    "print('Vocab size:', VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f43ff0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 23)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_H, MAX_W = 30, 30\n",
    "\n",
    "def to_2d(grid):\n",
    "    if grid is None:\n",
    "        return []\n",
    "    if isinstance(grid, (int, np.integer)):\n",
    "        return [[int(grid)]]\n",
    "    if isinstance(grid, list):\n",
    "        if not grid:\n",
    "            return []\n",
    "        if isinstance(grid[0], list):\n",
    "            return grid\n",
    "        else:\n",
    "            return [grid]\n",
    "    arr = np.array(grid)\n",
    "    if arr.ndim == 0:\n",
    "        return [[int(arr)]]\n",
    "    if arr.ndim == 1:\n",
    "        return [arr.astype(int).tolist()]\n",
    "    return arr.astype(int).tolist()\n",
    "class ArcSeqDataset(Dataset):\n",
    "    def __init__(self, samples, augment=False):\n",
    "        self.samples = samples\n",
    "        self.augment = augment\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        inp = to_2d(s.input)\n",
    "        out = to_2d(s.output)\n",
    "        if self.augment:\n",
    "            inp = apply_aug(inp)\n",
    "            out = apply_aug(out)\n",
    "        h_in, w_in = len(inp), len(inp[0]) if inp and len(inp) > 0 else 0\n",
    "        h_out, w_out = len(out), len(out[0]) if out and len(out) > 0 else 0\n",
    "        enc = encode_grid(inp)\n",
    "        dec_tgt_core = encode_grid(out)\n",
    "        dec_tgt = dec_tgt_core + [EOS]\n",
    "        dec_in = [BOS] + dec_tgt[:-1]\n",
    "        return {\n",
    "            'enc': torch.tensor(enc, dtype=torch.long),\n",
    "            'dec_in': torch.tensor(dec_in, dtype=torch.long),\n",
    "            'tgt': torch.tensor(dec_tgt, dtype=torch.long),\n",
    "            'h_in': h_in, 'w_in': w_in, 'h_out': h_out, 'w_out': w_out\n",
    "        }\n",
    "\n",
    "def make_row_col_indices(h, w):\n",
    "    rows = np.repeat(np.arange(h), w) if (h > 0 and w > 0) else np.array([], dtype=int)\n",
    "    cols = np.tile(np.arange(w), h) if (h > 0 and w > 0) else np.array([], dtype=int)\n",
    "    return rows, cols\n",
    "\n",
    "def collate_batch(batch):\n",
    "    B = len(batch)\n",
    "    enc_lens = [len(b['enc']) for b in batch]\n",
    "    dec_lens = [len(b['dec_in']) for b in batch]\n",
    "    max_enc = max(enc_lens) if enc_lens else 0\n",
    "    max_dec = max(dec_lens) if dec_lens else 0\n",
    "    enc = torch.full((B, max_enc), PAD, dtype=torch.long)\n",
    "    dec_in = torch.full((B, max_dec), PAD, dtype=torch.long)\n",
    "    tgt = torch.full((B, max_dec), PAD, dtype=torch.long)\n",
    "    enc_pad_mask = torch.ones((B, max_enc), dtype=torch.bool)\n",
    "    dec_pad_mask = torch.ones((B, max_dec), dtype=torch.bool)\n",
    "    row_idx = torch.zeros((B, max_enc), dtype=torch.long)\n",
    "    col_idx = torch.zeros((B, max_enc), dtype=torch.long)\n",
    "    meta = []\n",
    "    for i, b in enumerate(batch):\n",
    "        L_e = len(b['enc']); L_d = len(b['dec_in'])\n",
    "        enc[i, :L_e] = b['enc']\n",
    "        dec_in[i, :L_d] = b['dec_in']\n",
    "        tgt[i, :len(b['tgt'])] = b['tgt']\n",
    "        enc_pad_mask[i, :L_e] = False\n",
    "        dec_pad_mask[i, :L_d] = False\n",
    "        r, c = make_row_col_indices(b['h_in'], b['w_in'])\n",
    "        if L_e > 0 and len(r) == L_e:\n",
    "            row_idx[i, :L_e] = torch.tensor(r, dtype=torch.long)\n",
    "            col_idx[i, :L_e] = torch.tensor(c, dtype=torch.long)\n",
    "        meta.append((b['h_in'], b['w_in'], b['h_out'], b['w_out']))\n",
    "    return {\n",
    "        'enc': enc, 'dec_in': dec_in, 'tgt': tgt,\n",
    "        'enc_pad_mask': enc_pad_mask, 'dec_pad_mask': dec_pad_mask,\n",
    "        'row_idx': row_idx, 'col_idx': col_idx, 'meta': meta\n",
    "    }\n",
    "train_ds = ArcSeqDataset(train_samples, augment=True)\n",
    "val_ds = ArcSeqDataset(val_samples, augment=False)\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEM = (device.type == 'cuda')\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEM, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=PIN_MEM, collate_fn=collate_batch)\n",
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a50e8d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=4096):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0), persistent=False)\n",
    "    def forward(self, x):\n",
    "        L = x.size(1)\n",
    "        return x + self.pe[:, :L]\n",
    "class ArcTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, d_model=128, nhead=4, num_layers=3, dim_ff=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.row_emb = nn.Embedding(64, d_model)\n",
    "        self.col_emb = nn.Embedding(64, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_ff, dropout, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "    def encode(self, enc_tokens, row_idx, col_idx, src_key_padding_mask=None):\n",
    "        x = self.tok_emb(enc_tokens) + self.row_emb(row_idx) + self.col_emb(col_idx)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        return x\n",
    "    def decode(self, dec_tokens, memory, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        y = self.tok_emb(dec_tokens)\n",
    "        y = self.pos_enc(y)\n",
    "        L = y.size(1)\n",
    "        causal_mask = torch.triu(torch.ones(L, L, device=y.device, dtype=torch.bool), diagonal=1)\n",
    "        y = self.decoder(y, memory, tgt_mask=causal_mask,\n",
    "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                         memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return self.proj(y)\n",
    "    def forward(self, batch):\n",
    "        memory = self.encode(batch['enc'], batch['row_idx'], batch['col_idx'], src_key_padding_mask=batch['enc_pad_mask'])\n",
    "        logits = self.decode(batch['dec_in'], memory,\n",
    "                             tgt_key_padding_mask=batch['dec_pad_mask'],\n",
    "                             memory_key_padding_mask=batch['enc_pad_mask'])\n",
    "        return logits\n",
    "    @torch.no_grad()\n",
    "    def generate(self, enc, row_idx, col_idx, enc_pad_mask, max_len=256):\n",
    "        self.eval()\n",
    "        memory = self.encode(enc, row_idx, col_idx, src_key_padding_mask=enc_pad_mask)\n",
    "        B = enc.size(0)\n",
    "        ys = torch.full((B, 1), BOS, dtype=torch.long, device=enc.device)\n",
    "        for _ in range(max_len):\n",
    "            logits = self.decode(ys, memory,\n",
    "                                 tgt_key_padding_mask=torch.zeros_like(ys, dtype=torch.bool),\n",
    "                                 memory_key_padding_mask=enc_pad_mask)\n",
    "            next_tok = logits[:, -1].argmax(-1, keepdim=True)\n",
    "            ys = torch.cat([ys, next_tok], dim=1)\n",
    "            if (next_tok == EOS).all():\n",
    "                break\n",
    "        return ys[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfee4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.013774 M params\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aibel\\AppData\\Local\\Temp\\ipykernel_14172\\2723396765.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
     ]
    }
   ],
   "source": [
    "model = ArcTransformer(d_model=96, nhead=4, num_layers=2, dim_ff=192).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=8)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M params (small experiment model)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ce7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [train]: 100%|██████████| 202/202 [47:12<00:00, 14.02s/it, loss=1.09]\n",
      "Epoch 1 [val]:   0%|          | 0/23 [00:00<?, ?it/s]c:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "Epoch 1 [val]: 100%|██████████| 23/23 [00:48<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 val_loss=0.8768\n",
      "Saved new best to c:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\models\\run_20250824-092914\\best.pt\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5  # increased for small experiment\n",
    "ACCUM_STEPS = 1\n",
    "BEST_VAL = float('inf')\n",
    "best_path = RUN_DIR / 'best.pt'\n",
    "last_path = RUN_DIR / 'last.pt'\n",
    "train_history = []\n",
    "val_history = []\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch} [train]')\n",
    "    total_loss = 0.0\n",
    "    for step, batch in enumerate(pbar, 1):\n",
    "        for k in ['enc','dec_in','tgt','enc_pad_mask','dec_pad_mask','row_idx','col_idx']:\n",
    "            batch[k] = batch[k].to(device)\n",
    "        with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):\n",
    "            logits = model(batch)\n",
    "            B, L, V = logits.shape\n",
    "            loss = criterion(logits.view(B*L, V), batch['tgt'].view(B*L)) / ACCUM_STEPS\n",
    "        scaler.scale(loss).backward()\n",
    "        if step % ACCUM_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        total_loss += loss.item() * ACCUM_STEPS\n",
    "        pbar.set_postfix(loss=total_loss/step)\n",
    "    train_epoch_loss = total_loss / max(1, step)\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f'Epoch {epoch} [val]'):\n",
    "            for k in ['enc','dec_in','tgt','enc_pad_mask','dec_pad_mask','row_idx','col_idx']:\n",
    "                batch[k] = batch[k].to(device)\n",
    "            logits = model(batch)\n",
    "            B, L, V = logits.shape\n",
    "            loss = criterion(logits.view(B*L, V), batch['tgt'].view(B*L))\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= max(1, len(val_loader))\n",
    "    train_history.append(train_epoch_loss)\n",
    "    val_history.append(val_loss)\n",
    "    print(f'Epoch {epoch} train_loss={train_epoch_loss:.4f} val_loss={val_loss:.4f}')\n",
    "    torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch}, last_path)\n",
    "    if val_loss < BEST_VAL:\n",
    "        BEST_VAL = val_loss\n",
    "        torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch}, best_path)\n",
    "        print('Saved new best to', best_path)\n",
    "print('Training complete. Loss history:', {'train': train_history, 'val': val_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b073d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves for the small experiment\n",
    "if train_history and val_history:\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.plot(train_history, label='train_loss')\n",
    "    plt.plot(val_history, label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Experiment Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f968054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 23/23 [00:49<00:00,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 0.0, 'cell_accuracy': 0.7204714168517518}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    exact = 0\n",
    "    total = 0\n",
    "    cell_correct = 0\n",
    "    cell_total = 0\n",
    "    for batch in tqdm(loader, desc='Eval'):\n",
    "        for k in ['enc','dec_in','tgt','enc_pad_mask','dec_pad_mask','row_idx','col_idx']:\n",
    "            batch[k] = batch[k].to(device)\n",
    "        logits = model(batch)\n",
    "        preds = logits.argmax(-1)\n",
    "        mask = batch['tgt'] != PAD\n",
    "        equal = (preds == batch['tgt']) & mask\n",
    "        cell_correct += equal.sum().item()\n",
    "        cell_total += mask.sum().item()\n",
    "        seq_equal = (equal.sum(dim=1) == mask.sum(dim=1))\n",
    "        exact += seq_equal.sum().item()\n",
    "        total += preds.size(0)\n",
    "    return {\n",
    "        'exact_match': exact / max(1, total),\n",
    "        'cell_accuracy': cell_correct / max(1, cell_total)\n",
    "    }\n",
    "metrics = evaluate(model, val_loader)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Qualitative Predictions (Validation Samples) ===\n",
    "from random import sample as _sample\n",
    "model.eval()\n",
    "num_show = min(3, len(val_samples))\n",
    "show_examples = _sample(val_samples, num_show) if num_show > 0 else []\n",
    "print(f'Showing {len(show_examples)} validation examples:')\n",
    "for ex in show_examples:\n",
    "    grid_in = ex.input\n",
    "    target = ex.output\n",
    "    h_out, w_out = len(target), len(target[0]) if target else (0,0)\n",
    "    enc_tokens = torch.tensor(encode_grid(grid_in), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    h_in, w_in = len(grid_in), len(grid_in[0]) if grid_in else (0,0)\n",
    "    row_idx = torch.tensor([r for r in range(h_in) for _ in range(w_in)], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    col_idx = torch.tensor(list(range(w_in))*h_in, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    pad_mask = torch.zeros_like(enc_tokens, dtype=torch.bool)\n",
    "    max_len = h_out * w_out + 2\n",
    "    gen_seq = model.generate(enc_tokens, row_idx, col_idx, pad_mask, max_len=max_len)[0].cpu().tolist()\n",
    "    pred_grid = decode_grid(gen_seq, h_out, w_out)\n",
    "    print(f'Task {ex.task_id} subset {ex.subset} index {ex.index}')\n",
    "    print('Input :', grid_in)\n",
    "    print('Target:', target)\n",
    "    print('Pred  :', pred_grid)\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6189452",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def solve_batch(model, batch, max_len=256):\n",
    "    for k in ['enc','enc_pad_mask','row_idx','col_idx']:\n",
    "        batch[k] = batch[k].to(device)\n",
    "    gen = model.generate(batch['enc'], batch['row_idx'], batch['col_idx'], batch['enc_pad_mask'], max_len=max_len)\n",
    "    preds = gen.cpu().numpy().tolist()\n",
    "    outputs = []\n",
    "    for i, (h_in, w_in, h_out, w_out) in enumerate(batch['meta']):\n",
    "        outputs.append(decode_grid(preds[i], h_out, w_out))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c31b0179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 23/23 [00:47<00:00,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to c:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\models\\run_20250824-092914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'model': 'ArcTransformer', 'vocab_size': VOCAB_SIZE,\n",
    "    'd_model': 256, 'nhead': 8, 'num_layers': 4, 'dim_ff': 512,\n",
    "    'batch_size': BATCH_SIZE, 'epochs': EPOCHS, 'seed': SEED,\n",
    "}\n",
    "metrics = evaluate(model, val_loader)\n",
    "with (RUN_DIR / 'metrics.json').open('w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "torch.save({'model': model.state_dict(), 'config': config}, RUN_DIR / 'model.pt')\n",
    "with (RUN_DIR / 'config.json').open('w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print('Saved to', RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89c11329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode/Decode test passed\n",
      "Forward pass shape test passed\n"
     ]
    }
   ],
   "source": [
    "_grid = [[1,2,3],[4,5,6]]\n",
    "assert decode_grid(encode_grid(_grid), 2, 3) == _grid\n",
    "print('Encode/Decode test passed')\n",
    "batch = next(iter(train_loader))\n",
    "for k in ['enc','dec_in','tgt','enc_pad_mask','dec_pad_mask','row_idx','col_idx']:\n",
    "    batch[k] = batch[k].to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(batch)\n",
    "assert logits.shape[:2] == batch['tgt'].shape\n",
    "print('Forward pass shape test passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d32aeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def sweep(grid):\n",
    "    results = []\n",
    "    for (lr, layers, heads, dropout) in product(grid['lr'], grid['layers'], grid['heads'], grid['dropout']):\n",
    "        m = ArcTransformer(num_layers=layers, nhead=heads).to(device)\n",
    "        opt = torch.optim.AdamW(m.parameters(), lr=lr)\n",
    "        batch = next(iter(train_loader))\n",
    "        for k in ['enc','dec_in','tgt','enc_pad_mask','dec_pad_mask','row_idx','col_idx']:\n",
    "            batch[k] = batch[k].to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = m(batch)\n",
    "            B, L, V = logits.shape\n",
    "            loss = criterion(logits.view(B*L, V), batch['tgt'].view(B*L)).item()\n",
    "        results.append({'lr': lr, 'layers': layers, 'heads': heads, 'dropout': dropout, 'loss': loss})\n",
    "    return sorted(results, key=lambda x: x['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "952e2d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchScript export skipped: Tracer cannot infer type of ({'enc': tensor([[ 3,  5,  3,  ...,  5,  5,  3],\n",
      "        [ 9,  9,  2,  ...,  7,  9,  9],\n",
      "        [ 1,  9,  4,  ...,  2,  9,  1],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ..., 10, 10, 10],\n",
      "        [ 0,  4,  0,  ..., 10, 10, 10],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), 'dec_in': tensor([[11,  9,  9,  ..., 10, 10, 10],\n",
      "        [11,  3,  1,  ..., 10, 10, 10],\n",
      "        [11,  3,  7,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [11,  0,  0,  ..., 10, 10, 10],\n",
      "        [11,  0,  4,  ..., 10, 10, 10],\n",
      "        [11,  0,  0,  ...,  0,  0,  0]]), 'tgt': tensor([[ 9,  9,  6,  ..., 10, 10, 10],\n",
      "        [ 3,  1,  4,  ..., 10, 10, 10],\n",
      "        [ 3,  7,  4,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ..., 10, 10, 10],\n",
      "        [ 0,  4,  0,  ..., 10, 10, 10],\n",
      "        [ 0,  0,  0,  ...,  0,  0, 12]]), 'enc_pad_mask': tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ..., False, False, False]]), 'dec_pad_mask': tensor([[False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ..., False, False, False]]), 'row_idx': tensor([[ 0,  0,  0,  ..., 29, 29, 29],\n",
      "        [ 0,  0,  0,  ..., 29, 29, 29],\n",
      "        [ 0,  0,  0,  ..., 29, 29, 29],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ..., 29, 29, 29]]), 'col_idx': tensor([[ 0,  1,  2,  ..., 27, 28, 29],\n",
      "        [ 0,  1,  2,  ..., 27, 28, 29],\n",
      "        [ 0,  1,  2,  ..., 27, 28, 29],\n",
      "        ...,\n",
      "        [ 0,  1,  2,  ...,  0,  0,  0],\n",
      "        [ 0,  1,  2,  ...,  0,  0,  0],\n",
      "        [ 0,  1,  2,  ..., 27, 28, 29]]), 'meta': [(30, 30, 9, 4), (30, 30, 4, 5), (30, 30, 3, 7), (30, 30, 4, 4), (5, 13, 5, 13), (21, 22, 21, 22), (15, 15, 15, 7), (7, 15, 7, 7), (11, 15, 11, 7), (20, 20, 20, 20), (20, 20, 20, 20), (10, 13, 10, 13), (20, 20, 20, 20), (18, 18, 18, 18), (20, 20, 20, 20), (30, 30, 30, 30)]},)\n",
      ":Dictionary inputs to traced functions must have consistent type. Found Tensor and List[Tuple[int, int, int, int]]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    example = next(iter(val_loader))\n",
    "    for k in ['enc','dec_in','tgt','enc_pad_mask','dec_pad_mask','row_idx','col_idx']:\n",
    "        example[k] = example[k].to(device)\n",
    "    ts_path = RUN_DIR / 'model_ts.pt'\n",
    "    scripted = torch.jit.trace(model, (example))\n",
    "    scripted.save(str(ts_path))\n",
    "    print('Saved TorchScript to', ts_path)\n",
    "except Exception as e:\n",
    "    print('TorchScript export skipped:', e)\n",
    "try:\n",
    "    import onnx\n",
    "    onnx_path = RUN_DIR / 'model.onnx'\n",
    "    print('ONNX export not implemented in this baseline')\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "049f64a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval(gen):   9%|▊         | 2/23 [1:04:21<11:15:46, 1930.77s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     27\u001b[39m             total += \u001b[32m1\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     29\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mexact_match_seq\u001b[39m\u001b[33m'\u001b[39m: exact / \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, total),\n\u001b[32m     30\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcell_accuracy_seq\u001b[39m\u001b[33m'\u001b[39m: cell_correct / \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, cell_total)\n\u001b[32m     31\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m gen_metrics = \u001b[43mevaluate_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(gen_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mevaluate_generate\u001b[39m\u001b[34m(model, loader, max_len_factor)\u001b[39m\n\u001b[32m     13\u001b[39m gen_max = [\u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, h_out*w_out) + \u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (_, _, h_out, w_out) \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[33m'\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     14\u001b[39m max_len = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmax\u001b[39m(gen_max) * max_len_factor)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m gen = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_pad_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m preds = gen.cpu().tolist()\n\u001b[32m     17\u001b[39m tgt = batch[\u001b[33m'\u001b[39m\u001b[33mtgt\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mArcTransformer.generate\u001b[39m\u001b[34m(self, enc, row_idx, col_idx, enc_pad_mask, max_len)\u001b[39m\n\u001b[32m     50\u001b[39m ys = torch.full((B, \u001b[32m1\u001b[39m), BOS, dtype=torch.long, device=enc.device)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len):\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43menc_pad_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     next_tok = logits[:, -\u001b[32m1\u001b[39m].argmax(-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     56\u001b[39m     ys = torch.cat([ys, next_tok], dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mArcTransformer.decode\u001b[39m\u001b[34m(self, dec_tokens, memory, tgt_key_padding_mask, memory_key_padding_mask)\u001b[39m\n\u001b[32m     33\u001b[39m L = y.size(\u001b[32m1\u001b[39m)\n\u001b[32m     34\u001b[39m causal_mask = torch.triu(torch.ones(L, L, device=y.device, dtype=torch.bool), diagonal=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proj(y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:628\u001b[39m, in \u001b[36mTransformerDecoder.forward\u001b[39m\u001b[34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[39m\n\u001b[32m    625\u001b[39m tgt_is_causal = _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m     output = \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    640\u001b[39m     output = \u001b[38;5;28mself\u001b[39m.norm(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:1123\u001b[39m, in \u001b[36mTransformerDecoderLayer.forward\u001b[39m\u001b[34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[39m\n\u001b[32m   1120\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m._ff_block(\u001b[38;5;28mself\u001b[39m.norm3(x))\n\u001b[32m   1121\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1122\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm1(\n\u001b[32m-> \u001b[39m\u001b[32m1123\u001b[39m         x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1124\u001b[39m     )\n\u001b[32m   1125\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm2(\n\u001b[32m   1126\u001b[39m         x\n\u001b[32m   1127\u001b[39m         + \u001b[38;5;28mself\u001b[39m._mha_block(\n\u001b[32m   1128\u001b[39m             x, memory, memory_mask, memory_key_padding_mask, memory_is_causal\n\u001b[32m   1129\u001b[39m         )\n\u001b[32m   1130\u001b[39m     )\n\u001b[32m   1131\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm3(x + \u001b[38;5;28mself\u001b[39m._ff_block(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:1143\u001b[39m, in \u001b[36mTransformerDecoderLayer._sa_block\u001b[39m\u001b[34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sa_block\u001b[39m(\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1138\u001b[39m     x: Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1141\u001b[39m     is_causal: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1142\u001b[39m ) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1143\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m   1152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dropout1(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aibel\\OneDrive\\Documents\\Code\\arc-agi\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1320\u001b[39m, in \u001b[36mMultiheadAttention.forward\u001b[39m\u001b[34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   1315\u001b[39m         merged_mask, mask_type = \u001b[38;5;28mself\u001b[39m.merge_masks(\n\u001b[32m   1316\u001b[39m             attn_mask, key_padding_mask, query\n\u001b[32m   1317\u001b[39m         )\n\u001b[32m   1319\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.in_proj_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_native_multi_head_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m                \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m                \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m                \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmerged_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m                \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m                \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1336\u001b[39m any_nested = query.is_nested \u001b[38;5;129;01mor\u001b[39;00m key.is_nested \u001b[38;5;129;01mor\u001b[39;00m value.is_nested\n\u001b[32m   1337\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m any_nested, (\n\u001b[32m   1338\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMultiheadAttention does not support NestedTensor outside of its fast path. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1339\u001b[39m     + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe fast path was not hit because \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhy_not_fast_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1340\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_generate(model, loader, max_len_factor=1.2):\n",
    "    model.eval()\n",
    "    exact = 0\n",
    "    total = 0\n",
    "    cell_correct = 0\n",
    "    cell_total = 0\n",
    "    for batch in tqdm(loader, desc='Eval(gen)'):\n",
    "        enc = batch['enc'].to(device)\n",
    "        row_idx = batch['row_idx'].to(device)\n",
    "        col_idx = batch['col_idx'].to(device)\n",
    "        enc_pad_mask = batch['enc_pad_mask'].to(device)\n",
    "        gen_max = [max(1, h_out*w_out) + 1 for (_, _, h_out, w_out) in batch['meta']]\n",
    "        max_len = int(max(gen_max) * max_len_factor)\n",
    "        gen = model.generate(enc, row_idx, col_idx, enc_pad_mask, max_len=max_len)\n",
    "        preds = gen.cpu().tolist()\n",
    "        tgt = batch['tgt']\n",
    "        pad_mask = (tgt != PAD)\n",
    "        for i in range(tgt.size(0)):\n",
    "            tlen = pad_mask[i].sum().item()\n",
    "            p = preds[i][:tlen]\n",
    "            t = tgt[i, :tlen].tolist()\n",
    "            cell_total += tlen\n",
    "            cell_correct += sum(int(pi == ti) for pi, ti in zip(p, t))\n",
    "            if p == t:\n",
    "                exact += 1\n",
    "            total += 1\n",
    "    return {\n",
    "        'exact_match_seq': exact / max(1, total),\n",
    "        'cell_accuracy_seq': cell_correct / max(1, cell_total)\n",
    "    }\n",
    "gen_metrics = evaluate_generate(model, val_loader)\n",
    "print(gen_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f1ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d236390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== ArcSeqDataset ==\n",
      "Error getting ArcSeqDataset source: source code not available\n",
      "\n",
      "== collate_fn or collate ==\n",
      "\n",
      "-- collate_batch --\n",
      "\n",
      "def collate_batch(batch):\n",
      "    B = len(batch)\n",
      "    enc_lens = [len(b['enc']) for b in batch]\n",
      "    dec_lens = [len(b['dec_in']) for b in batch]\n",
      "    max_enc = max(enc_lens) if enc_lens else 0\n",
      "    max_dec = max(dec_lens) if dec_lens else 0\n",
      "\n",
      "    enc = torch.full((B, max_enc), PAD, dtype=torch.long)\n",
      "    dec_in = torch.full((B, max_dec), PAD, dtype=torch.long)\n",
      "    tgt = torch.full((B, max_dec), PAD, dtype=torch.long)\n",
      "\n",
      "    enc_pad_mask = torch.ones((B, max_enc), dtype=torch.bool)  # True for pad\n",
      "    dec_pad_mask = torch.ones((B, max_dec), dtype=torch.bool)\n",
      "\n",
      "    row_idx = torch.zeros((B, max_enc), dtype=torch.long)\n",
      "    col_idx = torch.zeros((B, max_enc), dtype=torch.long)\n",
      "\n",
      "    meta = []\n",
      "    for i, b in enumerate(batch):\n",
      "        L_e = len(b['enc']); L_d = len(b['dec_in'])\n",
      "        enc[i, :L_e] = b['enc']\n",
      "        dec_in[i, :L_d] = b['dec_in']\n",
      "        tgt[i, :len(b['tgt'])] = b['tgt']\n",
      "        enc_pad_mask[i, :L_e] = False\n",
      "        dec_pad_mask[i, :L_d] = False\n",
      "        r, c = make_row_col_indices(b['h_in'], b['w_in'])\n",
      "        if L_e > 0 and len(r) == L_e:\n",
      "            row_idx[i, :L_e] = torch.tensor(r, dtype=torch.long)\n",
      "            col_idx[i, :L_e] = torch.tensor(c, dtype=torch.long)\n",
      "        meta.append((b['h_in'], b['w_in'], b['h_out'], b['w_out']))\n",
      "\n",
      "    return {\n",
      "        'enc': enc, 'dec_in': dec_in, 'tgt': tgt,\n",
      "        'enc_pad_mask': enc_pad_mask, 'dec_pad_mask': dec_pad_mask,\n",
      "        'row_idx': row_idx, 'col_idx': col_idx, 'meta': meta\n",
      "    }\n",
      "\n",
      "\n",
      "== tokenization helpers (encode/decode) ==\n",
      "\n",
      "-- encode_grid --\n",
      "\n",
      "def encode_grid(grid: List[List[int]] | List[int] | int) -> List[int]:\n",
      "    arr = normalize_grid(grid)\n",
      "    return arr.reshape(-1).tolist()\n",
      "\n",
      "\n",
      "-- decode_grid --\n",
      "\n",
      "def decode_grid(tokens: List[int], h: int, w: int) -> List[List[int]]:\n",
      "    seq = tokens[: h*w]\n",
      "    return [seq[i*w:(i+1)*w] for i in range(h)]\n",
      "\n",
      "\n",
      "== model.generate ==\n",
      "    @torch.no_grad()\n",
      "    def generate(self, enc, row_idx, col_idx, enc_pad_mask, max_len=256):\n",
      "        self.eval()\n",
      "        memory = self.encode(enc, row_idx, col_idx, src_key_padding_mask=enc_pad_mask)\n",
      "        B = enc.size(0)\n",
      "        ys = torch.full((B, 1), BOS, dtype=torch.long, device=enc.device)\n",
      "        for _ in range(max_len):\n",
      "            logits = self.decode(ys, memory,\n",
      "                                 tgt_key_padding_mask=torch.zeros_like(ys, dtype=torch.bool),\n",
      "                                 memory_key_padding_mask=enc_pad_mask)\n",
      "            next_tok = logits[:, -1].argmax(-1, keepdim=True)\n",
      "            ys = torch.cat([ys, next_tok], dim=1)\n",
      "            if (next_tok == EOS).all():\n",
      "                break\n",
      "        return ys[:, 1:]  # drop BOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from textwrap import indent\n",
    "\n",
    "print(\"== ArcSeqDataset ==\")\n",
    "try:\n",
    "    print(inspect.getsource(ArcSeqDataset))\n",
    "except Exception as e:\n",
    "    print(\"Error getting ArcSeqDataset source:\", e)\n",
    "\n",
    "print(\"\\n== collate_fn or collate ==\")\n",
    "for name in list(globals().keys()):\n",
    "    if name.lower().startswith(\"collate\"):\n",
    "        obj = globals()[name]\n",
    "        if callable(obj):\n",
    "            print(f\"\\n-- {name} --\\n\")\n",
    "            try:\n",
    "                print(inspect.getsource(obj))\n",
    "            except Exception as e:\n",
    "                print(\"(no source)\", e)\n",
    "\n",
    "print(\"\\n== tokenization helpers (encode/decode) ==\")\n",
    "for name in [n for n in globals().keys() if any(n.lower().startswith(p) for p in (\"encode\",\"decode\",\"grid_to\",\"seq_to\"))]:\n",
    "    obj = globals()[name]\n",
    "    if callable(obj):\n",
    "        print(f\"\\n-- {name} --\\n\")\n",
    "        try:\n",
    "            print(inspect.getsource(obj))\n",
    "        except Exception as e:\n",
    "            print(\"(no source)\", e)\n",
    "\n",
    "print(\"\\n== model.generate ==\")\n",
    "try:\n",
    "    print(inspect.getsource(model.generate))\n",
    "except Exception as e:\n",
    "    print(\"Error getting model.generate source:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2926c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample 0:\n",
      "keys: ['enc', 'dec_in', 'tgt', 'h_in', 'w_in', 'h_out', 'w_out']\n",
      "enc: <class 'torch.Tensor'> -> tensor([3, 9, 7, 4])\n",
      "dec_in: <class 'torch.Tensor'> -> tensor([11,  9,  3,  9,  3,  9,  3,  7,  4,  7,  4,  7,  4,  3,  9,  3,  9,  3,\n",
      "         9,  4,  7,  4,  7,  4,  7,  9,  3,  9,  3,  9,  3,  7,  4,  7,  4,  7,\n",
      "         4])\n",
      "tgt: <class 'torch.Tensor'> -> tensor([ 9,  3,  9,  3,  9,  3,  7,  4,  7,  4,  7,  4,  3,  9,  3,  9,  3,  9,\n",
      "         4,  7,  4,  7,  4,  7,  9,  3,  9,  3,  9,  3,  7,  4,  7,  4,  7,  4,\n",
      "        12])\n",
      "h_in 2\n",
      "w_in 2\n",
      "h_out 6\n",
      "w_out 6\n",
      "\n",
      "Val sample 0:\n",
      "keys: ['enc', 'dec_in', 'tgt', 'h_in', 'w_in', 'h_out', 'w_out']\n",
      "enc: <class 'torch.Tensor'> -> tensor([3, 5, 3, 3, 6, 6, 5, 4, 1, 4, 9, 9, 4, 3, 9, 9, 9, 9, 3, 4, 9, 9, 4, 1,\n",
      "        4, 5, 6, 6, 3, 3, 5, 3, 3, 3, 6, 6, 4, 5, 4, 1, 9, 9, 3, 4, 9, 1, 1, 9,\n",
      "        4, 3, 9, 9, 1, 4, 5, 4, 6, 6, 3, 3, 1, 1, 3, 5, 5, 4, 6, 6, 9, 1, 1, 4,\n",
      "        9, 9, 4, 5, 5, 4, 9, 9, 4, 1, 1, 9, 6, 6, 4, 5, 5, 3, 1, 1, 5, 3, 4, 5,\n",
      "        6, 6, 1, 9, 4, 1, 9, 1, 4, 4, 4, 4, 1, 9, 1, 4, 9, 1, 6, 6, 5, 4, 3, 5,\n",
      "        6, 9, 9, 9, 3, 5, 3, 3, 4, 3, 9, 9, 9, 2, 6, 9, 9, 6, 2, 9, 9, 9, 3, 4,\n",
      "        3, 3, 5, 3, 9, 9, 9, 6, 9, 9, 5, 3, 3, 3, 3, 4, 9, 1, 9, 9, 9, 6, 6, 9,\n",
      "        9, 9, 1, 9, 4, 3, 3, 3, 3, 5, 9, 9, 9, 9, 6, 9, 1, 1, 3, 5, 9, 9, 4, 4,\n",
      "        6, 9, 9, 2, 2, 9, 9, 6, 4, 4, 9, 9, 5, 3, 1, 1, 9, 6, 9, 9, 9, 6, 1, 1,\n",
      "        5, 3, 9, 1, 5, 4, 9, 6, 9, 9, 9, 9, 6, 9, 4, 5, 1, 9, 3, 5, 1, 1, 6, 9,\n",
      "        1, 4, 9, 1, 4, 3, 9, 9, 5, 5, 7, 2, 4, 3, 2, 4, 4, 2, 3, 4, 2, 7, 5, 5,\n",
      "        9, 9, 3, 4, 1, 9, 4, 1, 1, 9, 3, 4, 9, 1, 4, 5, 2, 7, 3, 4, 4, 2, 2, 4,\n",
      "        4, 3, 7, 2, 5, 4, 1, 9, 4, 3, 9, 1, 9, 9, 1, 4, 9, 9, 4, 5, 6, 4, 5, 5,\n",
      "        2, 4, 4, 3, 3, 4, 4, 2, 5, 5, 4, 6, 5, 4, 9, 9, 4, 1, 9, 9, 4, 1, 9, 1,\n",
      "        4, 4, 4, 5, 4, 5, 4, 2, 3, 4, 4, 3, 2, 4, 5, 4, 5, 4, 4, 4, 1, 9, 1, 4,\n",
      "        4, 3, 9, 9, 9, 9, 6, 9, 5, 9, 7, 7, 5, 5, 7, 2, 2, 7, 5, 5, 7, 7, 9, 5,\n",
      "        9, 6, 9, 9, 9, 9, 3, 4, 9, 1, 2, 9, 9, 6, 9, 5, 7, 7, 4, 5, 2, 7, 7, 2,\n",
      "        5, 4, 7, 7, 5, 9, 6, 9, 9, 2, 1, 9, 9, 9, 4, 4, 6, 9, 9, 9, 7, 7, 5, 9,\n",
      "        5, 4, 5, 5, 5, 5, 4, 5, 9, 5, 7, 7, 9, 8, 8, 8, 8, 4, 9, 1, 5, 4, 9, 6,\n",
      "        2, 9, 7, 7, 9, 5, 4, 6, 4, 5, 5, 4, 6, 4, 5, 9, 7, 7, 9, 8, 8, 8, 8, 5,\n",
      "        9, 1, 5, 4, 9, 6, 2, 9, 7, 7, 9, 5, 4, 6, 4, 5, 5, 4, 6, 4, 5, 9, 7, 7,\n",
      "        9, 8, 8, 8, 8, 5, 9, 9, 4, 4, 6, 9, 9, 9, 7, 7, 5, 9, 5, 4, 5, 5, 5, 5,\n",
      "        4, 5, 9, 5, 7, 7, 9, 8, 8, 8, 8, 4, 3, 4, 9, 1, 2, 9, 9, 6, 9, 5, 7, 7,\n",
      "        4, 5, 2, 7, 7, 2, 5, 4, 7, 7, 5, 9, 6, 8, 8, 8, 8, 9, 4, 3, 9, 9, 9, 9,\n",
      "        6, 9, 5, 9, 7, 7, 5, 5, 7, 2, 2, 7, 5, 5, 7, 7, 9, 5, 9, 8, 8, 8, 8, 9,\n",
      "        9, 9, 4, 1, 9, 1, 4, 4, 4, 5, 4, 5, 4, 2, 3, 4, 4, 3, 2, 4, 5, 4, 5, 4,\n",
      "        4, 8, 8, 8, 8, 4, 9, 9, 1, 4, 9, 9, 4, 5, 6, 4, 5, 5, 2, 4, 4, 3, 3, 4,\n",
      "        4, 2, 5, 5, 4, 6, 5, 8, 8, 8, 8, 1, 4, 1, 1, 9, 3, 4, 9, 1, 4, 5, 2, 7,\n",
      "        3, 4, 4, 2, 2, 4, 4, 3, 7, 2, 5, 4, 1, 8, 8, 8, 8, 1, 1, 4, 9, 1, 4, 3,\n",
      "        9, 9, 5, 5, 7, 2, 4, 3, 2, 4, 4, 2, 3, 4, 2, 7, 5, 5, 9, 9, 3, 4, 1, 9,\n",
      "        9, 9, 9, 6, 1, 1, 5, 3, 9, 1, 5, 4, 9, 6, 9, 9, 9, 9, 6, 9, 4, 5, 1, 9,\n",
      "        3, 5, 1, 1, 6, 9, 9, 9, 6, 9, 1, 1, 3, 5, 9, 9, 4, 4, 6, 9, 9, 2, 2, 9,\n",
      "        9, 6, 4, 4, 9, 9, 5, 3, 1, 1, 9, 6, 9, 6, 9, 9, 5, 3, 3, 3, 3, 4, 9, 1,\n",
      "        9, 9, 9, 6, 6, 9, 9, 9, 1, 9, 4, 3, 3, 3, 3, 5, 9, 9, 6, 9, 9, 9, 3, 5,\n",
      "        3, 3, 4, 3, 9, 9, 9, 2, 6, 9, 9, 6, 2, 9, 9, 9, 3, 4, 3, 3, 5, 3, 9, 9,\n",
      "        1, 1, 5, 3, 4, 5, 6, 6, 1, 9, 4, 1, 9, 1, 4, 4, 4, 4, 1, 9, 1, 4, 9, 1,\n",
      "        6, 6, 5, 4, 3, 5, 1, 1, 3, 5, 5, 4, 6, 6, 9, 1, 1, 4, 9, 9, 4, 5, 5, 4,\n",
      "        9, 9, 4, 1, 1, 9, 6, 6, 4, 5, 5, 3])\n",
      "dec_in: <class 'torch.Tensor'> -> tensor([11,  9,  9,  6,  4,  2,  6,  9,  4,  2,  6,  9,  4,  9,  9,  6,  4,  9,\n",
      "         9,  2,  1,  6,  9,  9,  9,  4,  1,  9,  1,  4,  9,  9,  4,  9,  4,  3,\n",
      "         9])\n",
      "tgt: <class 'torch.Tensor'> -> tensor([ 9,  9,  6,  4,  2,  6,  9,  4,  2,  6,  9,  4,  9,  9,  6,  4,  9,  9,\n",
      "         2,  1,  6,  9,  9,  9,  4,  1,  9,  1,  4,  9,  9,  4,  9,  4,  3,  9,\n",
      "        12])\n",
      "h_in 30\n",
      "w_in 30\n",
      "h_out 9\n",
      "w_out 4\n",
      "\n",
      "train_loader.collate_fn: <function collate_batch at 0x7f5bf52c4220>\n"
     ]
    }
   ],
   "source": [
    "def peek_sample(ds, idx=0):\n",
    "    s = ds[idx]\n",
    "    print(\"keys:\", list(s.keys()))\n",
    "    for k in (\"enc\",\"dec_in\",\"tgt\"):\n",
    "        v = s.get(k)\n",
    "        if isinstance(v, list):\n",
    "            print(f\"{k}: len={len(v)} head={v[:10]}\")\n",
    "        else:\n",
    "            print(f\"{k}: {type(v)} -> {v}\")\n",
    "    for k in (\"h_in\",\"w_in\",\"h_out\",\"w_out\"):\n",
    "        print(k, s.get(k))\n",
    "\n",
    "print(\"Train sample 0:\")\n",
    "peek_sample(train_ds, 0)\n",
    "\n",
    "print(\"\\nVal sample 0:\")\n",
    "peek_sample(val_ds, 0)\n",
    "\n",
    "print(\"\\ntrain_loader.collate_fn:\", getattr(train_loader, 'collate_fn', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c2b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len summaries:\n",
      "{'avg_dec_in': 134.4, 'avg_tgt': 134.4, 'avg_area': 133.4, 'eos_present_cnt': 20}\n",
      "First 3: [{'i': 0, 'dec_in': 37, 'tgt': 37, 'area': 36, 'has_eos': tensor(True)}, {'i': 1, 'dec_in': 37, 'tgt': 37, 'area': 36, 'has_eos': tensor(True)}, {'i': 2, 'dec_in': 82, 'tgt': 82, 'area': 81, 'has_eos': tensor(True)}]\n",
      "\n",
      "Val len summaries:\n",
      "{'avg_dec_in': 255.9, 'avg_tgt': 255.9, 'avg_area': 254.9, 'eos_present_cnt': 20}\n",
      "First 3: [{'i': 0, 'dec_in': 37, 'tgt': 37, 'area': 36, 'has_eos': tensor(True)}, {'i': 1, 'dec_in': 21, 'tgt': 21, 'area': 20, 'has_eos': tensor(True)}, {'i': 2, 'dec_in': 22, 'tgt': 22, 'area': 21, 'has_eos': tensor(True)}]\n"
     ]
    }
   ],
   "source": [
    "def has_eos(sample):\n",
    "    return len(sample['tgt']) > 0 and sample['tgt'][-1] == EOS\n",
    "\n",
    "# Inspect first 20 samples from train/val\n",
    "train_checks = []\n",
    "for i in range(min(20, len(train_ds))):\n",
    "    s = train_ds[i]\n",
    "    area = int(s['h_out']) * int(s['w_out'])\n",
    "    train_checks.append({\n",
    "        'i': i,\n",
    "        'dec_in': len(s['dec_in']),\n",
    "        'tgt': len(s['tgt']),\n",
    "        'area': area,\n",
    "        'has_eos': has_eos(s)\n",
    "    })\n",
    "\n",
    "val_checks = []\n",
    "for i in range(min(20, len(val_ds))):\n",
    "    s = val_ds[i]\n",
    "    area = int(s['h_out']) * int(s['w_out'])\n",
    "    val_checks.append({\n",
    "        'i': i,\n",
    "        'dec_in': len(s['dec_in']),\n",
    "        'tgt': len(s['tgt']),\n",
    "        'area': area,\n",
    "        'has_eos': has_eos(s)\n",
    "    })\n",
    "\n",
    "print('Train len summaries:')\n",
    "print({\n",
    "    'avg_dec_in': sum(x['dec_in'] for x in train_checks)/len(train_checks),\n",
    "    'avg_tgt': sum(x['tgt'] for x in train_checks)/len(train_checks),\n",
    "    'avg_area': sum(x['area'] for x in train_checks)/len(train_checks),\n",
    "    'eos_present_cnt': sum(1 for x in train_checks if x['has_eos'])\n",
    "})\n",
    "print('First 3:', train_checks[:3])\n",
    "\n",
    "print('\\nVal len summaries:')\n",
    "print({\n",
    "    'avg_dec_in': sum(x['dec_in'] for x in val_checks)/len(val_checks),\n",
    "    'avg_tgt': sum(x['tgt'] for x in val_checks)/len(val_checks),\n",
    "    'avg_area': sum(x['area'] for x in val_checks)/len(val_checks),\n",
    "    'eos_present_cnt': sum(1 for x in val_checks if x['has_eos'])\n",
    "})\n",
    "print('First 3:', val_checks[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576807a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best checkpoint: /home/aibe/Documents/Code/arc-agi/models/run_20250809-185104/best.pt\n",
      "Generate eval (val): {'exact_match': 0.0, 'cell_accuracy': 0.27584441142214544, 'n_examples': 358}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_best(model, path):\n",
    "    if os.path.exists(path):\n",
    "        sd = torch.load(path, map_location=device)\n",
    "        if isinstance(sd, dict) and 'model' in sd:\n",
    "            model.load_state_dict(sd['model'])\n",
    "        else:\n",
    "            model.load_state_dict(sd)\n",
    "        print(f\"Loaded best checkpoint: {path}\")\n",
    "    else:\n",
    "        print(f\"Best checkpoint not found: {path}\")\n",
    "load_best(model, str(best_path))\n",
    "@torch.no_grad()\n",
    "def evaluate_generate(model, loader, max_gen_len=None, limit=None):\n",
    "    model.eval()\n",
    "    total_cells = 0\n",
    "    correct_cells = 0\n",
    "    exact_matches = 0\n",
    "    total_examples = 0\n",
    "    for bi, batch in enumerate(loader):\n",
    "        enc = batch['enc'].to(device)\n",
    "        enc_pad = batch['enc_pad_mask'].to(device)\n",
    "        row_idx = batch['row_idx'].to(device)\n",
    "        col_idx = batch['col_idx'].to(device)\n",
    "        tgt = batch['tgt'].to(device)\n",
    "        B = enc.size(0)\n",
    "        metas = batch['meta']\n",
    "        areas = [int(h_out) * int(w_out) for (_, _, h_out, w_out) in metas]\n",
    "        max_area = max(areas) if areas else 0\n",
    "        max_len = max_gen_len or (max_area + 2)\n",
    "        ys = model.generate(enc, row_idx, col_idx, enc_pad, max_len=max_len)\n",
    "        for i in range(B):\n",
    "            area = areas[i]\n",
    "            tgt_seq = tgt[i, :area]\n",
    "            pred = ys[i]\n",
    "            eos_pos = (pred == EOS).nonzero(as_tuple=False)\n",
    "            if len(eos_pos) > 0:\n",
    "                pred = pred[: int(eos_pos[0].item())]\n",
    "            pred_area = pred[:area]\n",
    "            if pred_area.numel() < area:\n",
    "                pad = torch.full((area - pred_area.numel(),), PAD, dtype=pred_area.dtype, device=pred_area.device)\n",
    "                pred_area = torch.cat([pred_area, pad], dim=0)\n",
    "            else:\n",
    "                pred_area = pred_area[:area]\n",
    "            correct = (pred_area == tgt_seq).sum().item()\n",
    "            correct_cells += correct\n",
    "            total_cells += area\n",
    "            if correct == area and pred.numel() >= area:\n",
    "                exact_matches += 1\n",
    "            total_examples += 1\n",
    "        if limit and total_examples >= limit:\n",
    "            break\n",
    "    return {\n",
    "        'exact_match': exact_matches / max(1, total_examples),\n",
    "        'cell_accuracy': correct_cells / max(1, total_cells),\n",
    "        'n_examples': total_examples,\n",
    "    }\n",
    "metrics_gen = evaluate_generate(model, val_loader, max_gen_len=MAX_H*MAX_W+2)\n",
    "print(\"Generate eval (val):\", metrics_gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
